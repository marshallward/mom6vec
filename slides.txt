=========================
The Vectorization of MOM6
=========================

:author: Marshall Ward
:description: Overview of recent efforts to vectorize MOM6
:date: 2021-04-21
:url: https://marshallward.org/ogrp2021/index.html
:preface:
   Introductory comments, written to slide notes


Peak Performance
================

.. math::

   W \le N \times f \times I_\text{vec}

====================    =========================
:math:`W`               FLOPs per second ("work")
:math:`N`               # of CPUs
:math:`f`               Cycles per second
:math:`I_\text{vec}`    FLOPs per cycle
====================    =========================

.. notes::

   * "Iron law" of computing

   * Usually written as time, but inverse form is better for us.

   * "Work" here is floating point ops per second.

   * Just like real work, they are not necessarily all useful.

   The first two terms are very straightforward: N is the number of CPUs and f
   is the clock rate, or cycles per second.  More CPUs and faster clock rate
   means more work.

   The third term here is more interesting, and is the focus here.  This
   represents the ability of the CPU to do concurrent work on a single core.


CPU Scaling
===========

.. image:: img/scaling_ocn.svg

MOM5 scaling in ACCESS-OM2

.. notes::

   * I arguably made a career out of producing plots like the one shown here.

   * CPU scaling has become traditional source of improvement, but it has its
     limits.

   * Ocean models are uniquely suited to scaling due to locality

   * Runtimes for MOM5 in the ACCESS-OM2 coupled model are shown.

   * Left figure shows that the model is scaleable across resolutions, but tens
     to taper off at ~0.1 sec/step.

   * 0.1deg hit the machine limit (well, limit I was allowed to use, at least)

   * Probably would not have hit 0.1s but might have gotten closer.

   * HOWEVER, right hand side shows that the work per step was still going up
     and up, even if time per step was not.

   * As resolutions drop, CFL means more work per step!

   * If dt/step is capped, and steps are getting smaller, is this really
     sustainable?


Clock Speed
===========

.. image:: img/clockspeed.svg
   :width: 70%

.. notes::

   * Clock speed is the former traditional source of improvement.

   * But clock speed hasn't really changed since ~2006.

   * This is from CPUDB which stops at 2014, but situation has not really
     changed.

   * Dennard scaling (constant power per area) presented a feasible path to
     improvement:

      * Shrink area, raise f, and power consumption stays the same

   * Until 2006 and we're back to nowhere.


Vectorization
=============

.. image:: img/specfp.svg

.. notes:::

   * Despite the end of clock speed increase, FP rates continue to rise

   * This is the SPECfp benchmark, it's a bit of a mash of various FP codes,
     but does include some Fortran models.

      * This is actually a composite of four SPECfp benchmarks, normalized
        across Intel CPUs.

   * Main point: despite halt of CPU speeds (denoted by dash) FP performance
     did continuee to rise, albeit more slowly.

   * Less important: What happened in the 90s?

      * All the DEC, SPARC, etc vector CPUs.  They failed to keep up with x86.


Intel normalization
-------------------

.. image:: img/specfp_intel.svg

.. notes::

   Showing the normalization of Intel-based architectures.


How to improve performance?
===========================

CPU scaling
   Viable but CFL-limited

Clock speed
   Unchanged for >15 years

Vectorization
   Potential speedup?


Vector Instructions
===================

.. image:: img/avx.svg

======   ==========  =======  ===========
Instr.   Size        GFLOP/s  Obs. (Gaea)
======   ==========  =======  ===========
SSE      2 doubles   7.2      7.196
AVX      4 doubles   14.4     14.395
======   ==========  =======  ===========

.. notes::

   * Doube precision!  Not single.

   * Gaea C4 (Xeon E5-2697 v4, 'Broadwell')

   * :math:`f_\text{peak} = 3.6 \times 10^9 \ \text{Hz}`


Fused-Multiply Add
==================

:math:`d \leftarrow a \times b + c`

======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         7.2      7.196

         FMA         14.4     14.394

AVX      Add         14.4     14.395

         FMA         28.8     28.788
======   ==========  =======  =======

:math:`\text{rd}(\text{rd}(a \times b) + c)`
vs :math:`\text{rd}(a \times b + c)`

.. notes::

   * FMAs gave a boost to certain types of computation, esp. matrix arithmetic.

   * Also offer speedy dot products

   * Note they *change answers*!  Rounding rules are technically more accurate.


Concurrency
===========

.. image:: img/broadwell_exc.svg
   :ref: https://en.wikichip.org/wiki/File:broadwell_block_diagram.svg

2 instr. per cycle, even FMA

.. notes::

   * Modern CPUs can execute more than one instruction per cycle.

   * Broadwells can do two adds, muls, or FMAs at a time

   * Non-arithmetic stuff like loop increments are usually concurrent


Peak
====

======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         7.2      7.196

         FMA         14.4     14.394

         2x FMA      28.8     28.788

AVX      Add         14.4     14.395

         Add+Mul     28.8     28.788

         FMA         28.8     28.788

         2x FMA      57.6     57.576
======   ==========  =======  =======


Array Performance
=================

.. image:: img/gaea_dp_flops.svg

.. notes::

   * Loops of simple array updates are shown here.

   * No assembly, this is in C and the compiler is autovectorizing.

   Lots of detail here, but main points

   * For small arrays, there is a lot of diversity in performance.
     Not going to get into that!

   * As arrays get larger two things happen:

      * Performance drops like a stone in steps

      * Most results collapse onto the lowest curve


AMD Arrays
----------

.. image:: img/zen3_dp_flops.svg

   This was a surprising result to me, and I am just giving you my
   interpretation here.  I have not yet read any specs explaining this.

   * There is no "drop" in levels!  L1-L3 is about the same

      * This is presumably better cache management.  Could be a Zen 3 thing,
        and maybe newer Intels do this?

   * Otherwise the same one you're RAM bound


Single Precision
----------------

.. image:: img/gaea_flops.svg


Array Cacheing
==============

.. image:: img/gaea_dp_flops_cached.svg

.. notes::

   * Cache hierarchy explains the regimes of performance

   * Each line indicates the array length of exactly two arrays in the cache.

AMD Cacheing
------------

.. image:: img/zen3_dp_flops_cached.svg


Memory-bound
============

.. image:: img/gaea_dp_flops_membnd.svg

.. notes::

   More detail of the point made earlier.

   See next slide for the actual numbers:


Peak Array Ops
--------------

======================  ====  ====  ====  ====
Expression              Max   L2    L3    DRAM
======================  ====  ====  ====  ====
y[:] = a x[:]           11.1  3.5   1.8   0.7
y[:] = x[:] + x[:]      12.3  3.6   1.8   0.7
y[:] = x[:] + y[:]      9.3   3.5   1.8   0.7
y[:] = a x[:] + y[:]    18.3  7.0   3.6   1.4
y[:] = a x[:] + b y[:]  23.9  10.6  5.5   2.0
y[:] = x[1:] - x[:-1]   7.0   3.4   1.8   0.7
y[:] = x[8:] - x[:-8]   9.3   3.5   1.8   0.7
======================  ====  ====  ====  ====


MOM6 sample config
==================

.. image:: img/benchmark_topo.svg
   :class: float
   :width: 35%

* 32 × 32 grid, 75 level

  - ~76k / field (~:math:`2^{16}`)
  - :math:`2^{10}` per level

* 288 steps (3 day, :math:`\Delta t = 900s`)

* "Benchmark" configuration:

  - Split barotropic

  - Smagorinsky biharmonic

  - Thermo, Wright EOS

  - Bounded Coriolis terms

  - Layered (no ALE)

.. notes::

   * Not necessarily the most physically signficant factors.  Just the ones
     which had a significant impact on performance.

   * Also note that this domain has a nontrivial land mask.

     This will be useful later for testing masked array loops.


Profiling with perf
===================

.. image:: img/perf_symbols.png
   :width: 80%


Profiling with perf
===================

.. image:: img/perf_lines.png
   :width: 80%


perf stat
=========

.. include:: perf.stat
   :code:


Profile Overview
================

.. image:: img/subroutine_profile_gaea.svg

.. notes::

   * Hopefully not too confusing... %runtime and FLOPS together, even same axis

   * Speeds are on order of 1-2 GFLOP/s

   * Places most results in DRAM or L3 bound

   * EOS is doing quite well at >10 GFLOP/s

   * Also note the "long tail" in the top 50 functions.. there are no easy wins
     here, it will take a lot of improvements over a lot of functions to see
     any major speedups.


AMD Profile
-----------

.. image:: img/subroutine_profile.svg

.. notes::

   * btstep takes a bigger hit on AMD

   * EOS is a bit slower but still dominant

   * Speeds are a bit higher, reflecting better L2/L3 performance


Investigation Targets
=====================

* Horizontal viscosity

* Coriolis force advection (``coradcalc``)

* Barotropic solver (``btstep``)

* Vertical viscosity (``find_coupling_coef``)

* EOS (``int_density_dz_wright``)


Horizontal Viscosity
====================

.. include:: src/MOM_hor_visc.F90
   :code: fortran
   :start-line: 831
   :end-line: 958
   :number-lines: 831
   :data-line-numbers: 1|2-6|7-13|14-18|20-73|127


Non-vectorized code
===================

.. code:: x86asm

   │      │833    Shear_mag = sqrt(sh_xx(i,j)*sh_xx(i,j) + &
   │ 0.70 │         vaddsd   %xmm13,%xmm12,%xmm14
   │ 1.62 │         vsqrtsd  %xmm14,%xmm14,%xmm14
   │ 6.53 │         vmovsd   %xmm14,-0x8e8(%rbp)


==========  ========
``v___sd``  Serial
``v___pd``  Parallel
==========  ========


Excessive Stack
===============

.. code:: x86asm

   │      │919    Ah = MAX(MAX(CS%Ah_bg_xx(i,j), AhSm), AhLth)
   │      │         lea      (%rax,%rdx,8),%rdi
   │      │         lea      (%rdi,%rsi,1),%r8
   │ 0.39 │         vmovsd   (%r8,%r9,8),%xmm0
   │ 0.01 │         vmaxsd   -0x13f8(%rbp),%xmm0,%xmm0
   │ 2.34 │         vmaxsd   -0x13f0(%rbp),%xmm0,%xmm0
   │ 0.42 │         vmovsd   %xmm0,-0x1468(%rbp)

======   ===================
lea      Compute mem address
vmovsd   Serial move
vmaxsd   Serial max
======   ===================


Vectorized HorVisc
==================

.. include:: src/MOM_hor_visc_vec.F90
   :code: fortran
   :start-line: 854
   :end-line: 1128
   :number-lines: 831
   :data-line-numbers: 1-10|12-23|28-32|225-231

1. Promote repeated scalars to 2d arrays

2. Move if-blocks outside loops


Hor Visc Speedup
=====================

==========  ======   ======   =======
Platform    Old      New      Speedup
==========  ======   ======   =======
Gaea C4     2.23s    1.27s    1.75x

            (1.67)   (2.93)

AMD @ home  1.69s    1.01s    1.67x

            (2.30)   (3.76)
==========  ======   ======   =======


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv.F90
   :code: fortran
   :start-line: 424
   :end-line: 686
   :number-lines: 424
   :data-line-numbers: 1|17-32|245-257


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv_vec.F90
   :code: fortran
   :start-line: 669
   :end-line: 694
   :number-lines: 669
   :data-line-numbers: 7-20


Coriolis Speedup
================

========    ======   ======   =======
Platform    Old      New      Speedup
========    ======   ======   =======
Gaea        1.06     0.87     1.23x

            (2.79)   (3.43)

AMD         1.11     0.64     1.74x

            (2.68)   (5.21)
========    ======   ======   =======


Barotropic Optimization
=======================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(1491,26)
      remark #15389: vectorization support: reference eta_wtd(i,j) has unaligned access   [ ../      ../ac/../src/core/MOM_barotropic.F90(1492,7) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 2
      remark #15309: vectorization support: normalized vectorization overhead 0.300
      remark #15300: LOOP WAS VECTORIZED
      remark #15451: unmasked unaligned unit stride stores: 1
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4
      remark #15477: vector cost: 2.500
      remark #15478: estimated potential speedup: 1.450
      remark #15488: --- end vector cost summary ---
      remark #25015: Estimate of max trip count of loop=3
   LOOP END


Barotropic Optimization...?
===========================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(2383,39)
      remark #25460: No loop optimizations reported
   LOOP END

.. code::

   remark #25464: Some optimizations were skipped to constrain compile
      time. Consider overriding limits (-qoverride-limits).


Barotropic Speedup
==================

Engage the Override!

========    ======   ======   =======
Platform    Old      New      Speedup
========    ======   ======   =======
Gaea        2.35     1.60     1.46x

            (1.19)   (1.74)

AMD         3.13     1.66     1.89x

            (0.89)   (1.66)
========    ======   ======   =======

Sustainable solution is to reduce size of ``btstep()``


Vertical Viscosity
==================

.. include:: src/MOM_vert_friction.F90
   :code: fortran
   :start-line: 1248
   :end-line: 1270
   :number-lines: 1248
   :data-line-numbers: 1-22|1|22

.. notes::

   The ``do_I(:)`` arrays are used to skip over land.  These can impede
   vectorization.


Vertical Viscosity
==================

.. include:: src/MOM_vert_friction_vec.F90
   :code: fortran
   :start-line: 1462
   :end-line: 1489
   :number-lines: 1462
   :data-line-numbers: 1-4|23-27

.. notes::

   So we remove them, then multiply by i_mask when writing a_cpl.

   This lets us keep moving contiguously though memory, and set up an
   uninterrupted pipeline.


Maintain Pipelines
==================

.. image:: img/stay_on_target.gif
   :width: 45%

Don't stop, keep going, even over land!


Vertical Viscosity speedup?
===========================

FLOPs are up!

==================   ====   ====
Subroutine           Old    New
==================   ====   ====
vertvisc_coef        1.29   2.00
find_coupling_coef   2.50   5.72
==================   ====   ====


Vertical Viscosity "speedup"
============================

Runtimes, not quite as exciting.

==================   =====    =====    =======
Subroutine           Old      New      Speedup
==================   =====    =====    =======
vertvisc_coef        1.54s    1.50s    😐
find_coupling_coef   0.82s    0.68s    1.21x
==================   =====    =====    =======

.. notes::

   Though to tell the truth... the results were less than impressive.

   In practice, this could end up being better or worse, depending on the
   amount of land.

   Which is why I haven't submitted this change.  But it is interesting an
   worth revisiting soon.


What about that EOS?
====================

.. include:: src/MOM_EOS_Wright.F90
   :code: fortran
   :start-line: 578
   :end-line: 619
   :number-lines: 578

High *arithmetic intensity*: many ops per memory access


Gaea Speedup
============

.. image:: img/subroutine_speedup_gaea.svg

.. notes::

   So several subroutines had significant speedup, maybe about 2x in some
   cases, a fair bit less in others.

   Total improvement was maybe ~10%

   But the long tail was always going to make it a long fight to get any
   speedup.  The real goal here was to lay down the rules to maintain efficient
   code.

   In that sense, I think this project has been useful and ought to yield more
   improvements in the future.


AMD Speedup
-----------

.. image:: img/subroutine_speedup.svg


Moving Forward
==============

Development guidelines:

* Profile and identify hotspots

* Enable vectorization

* Avoid if-blocks in loops

* Keep references close together

* Reduce memory access

* Keep functions small (<2000 lines?)


Things we haven't talked about
------------------------------

* Array alignment and peel loops

* Heap vs Stack (incl. alignment)

* Pipelining

* Arithmetic Intensity


More things we haven't talked about
-----------------------------------

Multicore/node jobs

* Clock speed throttling

* Sharing the RAM bandwidth + NUMA

* perf on MPI jobs


Apocrypha
=========


Whence GPUs?
============

One core vs 1 GPU process

.. image:: img/p1_gpu_flops.svg
   :width: 60%


Whence GPUs?
------------

6 cores vs 6 GPU procs

.. image:: img/p6_gpu_flops.svg
   :width: 60%


Barotropic index reorder
========================

Lots of layered sums, e.g.

.. code:: fortran

   do j=js,je
      do k=1,nz
         do I=Isq,Ieq
            BT_force_u(I,j) = BT_force_u(I,j) &
                  + wt_u(I,j,k) * bc_accel_u(I,j,k)
         enddo
      enddo
   enddo

Very non-contiguous!  Stay on target!

Reorder to (k,i,j)


Barotropic kij Speedup
----------------------

.. image:: img/subroutine_speedup_bt.svg


Reordering problems
-------------------

1. Answer changes:

   .. math::

      (u_1 + u_2 + u_3 + u_4) + (u_5 + u_6 + u_7 + u_8) + ...

2. Transpose needed outside of loops!  Very slow...

   .. code:: fortran

      do k=1,nz
        u_tr(k,:,:) = u(:,:,k)
        v_tr(k,:,:) = v(:,:,k)
        ! ...
      enddo

Work in progress...


Ops per cycle
=============

This is slow::

   botfn_2d(i,k) = 1.0 / (1.0 + 0.09*z2*z2*z2*z2*z2*z2)

This is faster::

   z2_sq = z2 * z2
   botfn_2d(i,k) = 1.0 / (1.0 + 0.09 * (z2_sq * z2_sq * z2_sq))

1. Fewer operations (3 vs 5 mults)

2. Fewer instructions: Two can be pipelined!

But changes answers...


Heap vs Stack
=============

Stack
   Local memory created by the function

Heap
   External global memory, `allocate()`

Compilers cannot manipulate heap memory, must work around ambiguity.

Stack memory can be *aligned* for better performance.


Excessive Stack?
================

``horizontal_viscosity`` has 18 3D arrays on stack!  And over 2x as many 2D
arrays...

.. include:: src/MOM_hor_visc_decl.F90
   :code: fortran
   :start-line: 1
   :number-lines: 578

Moving declarations will slow model!  This is worrying...
