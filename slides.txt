=========================
The Vectorization of MOM6
=========================

:author: Marshall Ward
:description: Overview of recent efforts to vectorize MOM6
:date: 2021-04-21
:url: https://marshallward.org/ogrp2021.html
:preface:
   Introductory comments, written to slide notes


Peak Performance
================

.. math::

   W \le N \times f \times I_\text{vec}

====================    =========================
:math:`W`               FLOPs per second ("work")
:math:`N`               # of CPUs
:math:`f`               Cycles per second
:math:`I_\text{vec}`    FLOPs per cycle
====================    =========================

.. notes::

   This is a form of the so-called "iron law" of peak performance.

   It is normally written as an expression of time, rather than rate of work,
   but I think this inverse form is better suited for this discussion.

   The "work" here is written as the number of FLOPs (or floating point
   operations) per second, but it could be generalized to other units
   computational work.

   The first two terms are very straightforward: N is the number of CPUs and f
   is the clock rate, or cycles per second.  More CPUs and faster clock rate
   means more work.

   The third term here is more interesting, and is the focus here.  This
   represents the ability of the CPU to do concurrent work on a single core.


CPU Scaling
===========

.. image:: img/scaling_ocn.svg

MOM5 scaling in ACCESS-OM2


Fluid Dynamics Scaling
----------------------

Tension between local (hyperbolic) solvers

.. math::

   \frac{\partial \mathbf{u}}{\partial t}
      &= -\mathbf{u} \cdot \nabla \mathbf{u} - \nabla p + \mathbf{f} \\

and global (elliptic) solvers

.. math::

   \nabla^2 p
      &= -\nabla \cdot \left( \mathbf{u} \cdot \nabla \mathbf{u} \right)
         + \nabla \cdot \mathbf{F} \\
      &\approx \nabla \cdot \mathbf{F} \text{(?)} \\

... unless you can cheat (aka oceanography)


Clock Speed
===========

.. image:: img/clockspeed.svg
   :width: 70%

.. notes::

   This is a plot of CPU clock speeds from CPUDB.  The database goes from about
   1970 to 2014, but the situation has not changed much since then.

   The trend is clear:  Clock speed increased exponentially until about 2005.
   After that, it flattened out around 3GHz.  After that, people stopped
   talking much about clock speeds.

   This is a reflection of Moore's law (transistor rate) but is better
   explained by the breakdown of Dennard scaling.

   * P ~ f
   * P/A ~ const
   * transitor density drops exponentially => power drops too
   * => increase f to balance P

   After 2005, thermal leakage led to P >> A


Clock Speed vs # of Cores
-------------------------

Using more cores further reduces clock speed!


Vectorization
=============

.. image:: img/specfp.svg

.. notes:::

   Main comments:

   * Despite the end of clock speed increase, FP rates continue to rise

   * But after ~2004, the rate definitely drops.  What causes it?

   * Less important: What happened in the 90s?

     This was the era of high-end vector CPUs (DEC, SPARC, etc).
     As they failed to keep up with commodity x86 chips, they left the market.

   * Even less important: The "blob" at the end is emergence of mobile chips?

   What is SPECfp?

   This is a renormalized plot of the SPECfp scores over time.

   CPUs are assigned a "score" based on their performance of a suite of
   FP-based models.  The number is somewhat arbitrary.

   Unfortunately they periodically change these tests and SPEC strongly
   discourages comparison across tests, but in the words of Jurgen Willebrand,
   no one can stop me!

   We roughly normalize along Intel trends, especially 95->2k, which is why the
   "high end" cloud permeates.

   Don't take these numbers too literally, especially my renormalization which
   I didn't properly calibrate.


Intel normalization
-------------------

.. image:: img/specfp_intel.svg

.. notes::

   Showing the normalization of Intel-based architectures.


Vector Instructions
===================

.. image:: img/avx.svg

======   ==========  =======  ===========
Instr.   Size        GFLOP/s  Obs. (Gaea)
======   ==========  =======  ===========
SSE      4 SP/2 DP   14.4     14.394
AVX      8 SP/4 DP   28.8     28.790
======   ==========  =======  ===========

.. Gaea C4 (Xeon E5-2697 v4, "Broadwell")

.. :math:`f_\text{peak} = 3.6 \times 10^9 \ \text{Hz}`


Fused-Multiply Add
==================

FMA: :math:`d \leftarrow a \times b + c`

======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         14.4     14.394

         FMA         28.8     28.790

AVX      Add         28.8     28.788

         FMA         57.6     57.580
======   ==========  =======  =======

:math:`\text{rd}(\text{rd}(a \times b) + c)`
vs :math:`\text{rd}(a \times b + c)`



Concurrency
===========

.. image:: img/broadwell_exc.svg
   :ref: https://en.wikichip.org/wiki/File:broadwell_block_diagram.svg

2 instr. per cycle, even FMA


Peak
====


======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         14.4     14.394

         FMA         28.8     28.790

         2x FMA      57.6     57.580

AVX      Add         28.8     28.788

         Add+Mul     57.6     57.580

         FMA         57.6     57.580

         2x FMA      115.2    115.160
======   ==========  =======  =======

 
Array Ops
=========

.. image:: img/gaea_flops.svg


Array Ops
=========

.. image:: img/gaea_flops_cached.svg


Arrays @ AMD
============

.. image:: img/zen3_flops.svg


Arrays @ AMD
============

.. image:: img/zen3_flops_cached.svg


Arithmetic Intensity
====================

======================  ===================  ===========
Expression              AI                   Max GFLOP/s
======================  ===================  ===========
y[:] = a x[:]           :math:`\frac{1}{4}`  26.43
y[:] = x[:] + x[:]      :math:`\frac{1}{4}`  24.98
y[:] = x[:] + y[:]      :math:`\frac{1}{8}`  18.34
y[:] = a x[:] + y[:]    :math:`\frac{1}{4}`  37.58
y[:] = a x[:] + b y[:]  :math:`\frac{3}{8}`  51.08
y[:] = x[1:] - x[:-1]   :math:`\frac{1}{8}`  14.07
y[:] = x[8:] - x[:-8]   :math:`\frac{1}{8}`  18.39
======================  ===================  ===========


MOM6 sample config
==================

.. image:: img/benchmark_topo.svg
   :class: float
   :width: 35%

* 32 × 32 grid, 75 level

   - ~76k / field

* 288 steps (3 day, :math:`\Delta t = 900s`)

* "Benchmark" configuration:

   - Split barotropic

   - Biharmonic visc

   - Thickness diffusivity

   - Bounded Coriolis

   - ??


Profiling with perf
===================

.. image:: img/perf_symbols.png
   :width: 80%


Profiling with perf
===================

.. image:: img/perf_lines.png
   :width: 80%


perf stat
=========

.. include:: perf.stat
   :code:


MOM6 FLOP rates
===============

.. image:: img/subroutine_share.svg


Horizontal Viscosity
====================

.. include:: src/MOM_hor_visc.F90
   :code:
   :start-line: 831
   :end-line: 958
   :number-lines: 831
   :data-line-numbers: 1|2-6|7-13|14-18|20-73|127


Non-vectorized code
===================

.. code:: x86asm

   │      │833    Shear_mag = sqrt(sh_xx(i,j)*sh_xx(i,j) + &
   │ 0.70 │         vaddsd   %xmm13,%xmm12,%xmm14
   │ 1.62 │         vsqrtsd  %xmm14,%xmm14,%xmm14
   │ 6.53 │         vmovsd   %xmm14,-0x8e8(%rbp)


==========  ========
``v___sd``  Serial
``v___pd``  Parallel
==========  ========


Excessive Stack
===============

.. code:: x86asm

   │      │919    Ah = MAX(MAX(CS%Ah_bg_xx(i,j), AhSm), AhLth)
   │      │         lea      (%rax,%rdx,8),%rdi
   │      │         lea      (%rdi,%rsi,1),%r8
   │ 0.39 │         vmovsd   (%r8,%r9,8),%xmm0
   │ 0.01 │         vmaxsd   -0x13f8(%rbp),%xmm0,%xmm0
   │ 2.34 │         vmaxsd   -0x13f0(%rbp),%xmm0,%xmm0
   │ 0.42 │         vmovsd   %xmm0,-0x1468(%rbp)

======   ===================
lea      Compute mem address
vmovsd   Serial move
vmaxsd   Serial max
======   ===================


Hor Visc Speedup
=====================

============   ======   ======   =======
Platform       Old      New      Speedup
============   ======   ======   =======
Gaea C4        2.23s    1.27s    1.75x

               (1.67)   (2.93)   

Ryzen 5 2600   1.69s    1.01s    1.67x

               (2.30)   (3.76)   
============   ======   ======   =======


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv.F90
   :code:
   :start-line: 424
   :end-line: 686
   :number-lines: 424
   :data-line-numbers: 1|17-32|245-257


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv_vec.F90
   :code:
   :start-line: 669
   :end-line: 694
   :number-lines: 669
   :data-line-numbers: 7-20


Coriolis Speedup
================


===========    ======   ======   =======
Platform       Old      New      Speedup
===========    ======   ======   =======
Gaea           1.06     0.87     1.23x

               (2.79)   (3.43)   

AMD Ryzen 5    1.11     0.64     1.74x

               (2.68)   (5.21)   
===========    ======   ======   =======


Vertical Viscosity
==================

.. include:: src/MOM_vert_friction.F90
   :code:
   :start-line: 1248
   :end-line: 1270
   :number-lines: 1248
   :data-line-numbers: 1-23|1|23


Vertical Viscosity
==================

TODO: Remove ``do_I(:)``


Vertical Viscosity "speedup"
============================

Platform       GFLOP/s         Time
               Old   New      Old   New
Gaea

AMD Ryzen 5    1.29  2.00     1.54  1.50
               2.50  5.72     0.82  0.68


Barotropic Optimization
=======================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(1491,26)
      remark #15389: vectorization support: reference eta_wtd(i,j) has unaligned access   [ ../      ../ac/../src/core/MOM_barotropic.F90(1492,7) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 2
      remark #15309: vectorization support: normalized vectorization overhead 0.300
      remark #15300: LOOP WAS VECTORIZED
      remark #15451: unmasked unaligned unit stride stores: 1
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4
      remark #15477: vector cost: 2.500
      remark #15478: estimated potential speedup: 1.450
      remark #15488: --- end vector cost summary ---
      remark #25015: Estimate of max trip count of loop=3
   LOOP END



Barotropic Optimization...?
===========================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(2383,39)
      remark #25460: No loop optimizations reported
   LOOP END

.. code::

   remark #25464: Some optimizations were skipped to constrain compile
      time. Consider overriding limits (-qoverride-limits).


Barotropic Speedup
==================

Override limits!

===========    ======   ======   =======
Platform       Old      New      Speedup
===========    ======   ======   =======
Gaea           2.35     1.60     1.46x

               (1.19)   (1.74)

Ryzen 5        3.13     1.66     1.89x

               (0.89)   (1.66)
===========    ======   ======   =======

Sustainable solution is to reduce size of ``btstep()``


Barotropic Loop Order
=====================

* ijk -> kij vectorization

(Work in progress!!!)


Mystery Problems
================

* MOM_hor_visc and the stack
*


Hierarchy of Performance
========================

* Enable vectorization

   * Break large loops into smaller loops

   * No conditionals in loops
   
..  This sets up the pipeline: Repeat yourself as much as possible

* Reduce memory-per-loop

   * ``z[:] = a * x[:] + b * y[:]`` should ideally fit in L1

   * Loop should be pulling the next L2->L1 update during this loop

   * Eliminate RAM-bound operations (~2GFLOP/s)

* Reduce memory access

   * i.e. increase arithmetic intensity

   * Avoid deep out-of-order memory access

* Align arrays

  * In C, use posix_memalign, not malloc

  * Actually very difficult in Fortran!

  * Avoid "peel loops", i.e. overcommit your arrays

* Think about algorithm

  * No rules here, just avoid redundant work

* Software Engineering matters!

  * Keep functions small (<2000 lines?)

  * Keep stack variables small (< L2? L3?)
