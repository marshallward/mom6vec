=========================
The Vectorization of MOM6
=========================

:author: Marshall Ward
:description: Overview of recent efforts to vectorize MOM6
:date: 2021-04-21
:url: https://marshallward.org/ogrp2021.html
:preface:
   Introductory comments, written to slide notes


Peak Performance
================

.. math::

   W \le N \times f \times I_\text{vec}

====================    =========================
:math:`W`               FLOPs per second ("work")
:math:`N`               # of CPUs
:math:`f`               Cycles per second
:math:`I_\text{vec}`    FLOPs per cycle
====================    =========================

.. notes::

   * "Iron law" of computing

   * Usually written as time, but inverse form is better for us.

   * "Work" here is floating point ops per second.

   * Just like real work, they are not necessarily all useful.

   The first two terms are very straightforward: N is the number of CPUs and f
   is the clock rate, or cycles per second.  More CPUs and faster clock rate
   means more work.

   The third term here is more interesting, and is the focus here.  This
   represents the ability of the CPU to do concurrent work on a single core.


CPU Scaling
===========

.. image:: img/scaling_ocn.svg

MOM5 scaling in ACCESS-OM2

.. notes::

   * I arguably made a career out of producing plots like the one shown here.

   * CPU scaling has become traditional source of improvement, but it has its
     limits.

   * Ocean models are uniquely suited to scaling due to locality

   * Runtimes for MOM5 in the ACCESS-OM2 coupled model are shown.

   * Left figure shows that the model is scaleable across resolutions, but tens
     to taper off at ~0.1 sec/step.

   * 0.1deg hit the machine limit (well, limit I was allowed to use, at least)

   * Probably would not have hit 0.1s but might have gotten closer.

   * HOWEVER, right hand side shows that the work per step was still going up
     and up, even if time per step was not.

   * As resolutions drop, CFL means more work per step!

   * If dt/step is capped, and steps are getting smaller, is this really
     sustainable?


Clock Speed
===========

.. image:: img/clockspeed.svg
   :width: 70%

.. notes::

   * Clock speed is the former traditional source of improvement.

   * But clock speed hasn't really changed since ~2006.

   * This is from CPUDB which stops at 2014, but situation has not really
     changed.

   * Dennard scaling (constant power per area) presented a feasible path to
     improvement:

      * Shrink area, raise f, and power consumption stays the same

   * Until 2006 and we're back to nowhere.


Vectorization
=============

.. image:: img/specfp.svg

.. notes:::

   * Despite the end of clock speed increase, FP rates continue to rise

   * This is the SPECfp benchmark, it's a bit of a mash of various FP codes,
     but does include some Fortran models.

      * This is actually a composite of four SPECfp benchmarks, normalized
        across Intel CPUs.

   * Main point: despite halt of CPU speeds (denoted by dash) FP performance
     did continuee to rise, albeit more slowly.

   * Less important: What happened in the 90s?

      * All the DEC, SPARC, etc vector CPUs.  They failed to keep up with x86.


Intel normalization
-------------------

.. image:: img/specfp_intel.svg

.. notes::

   Showing the normalization of Intel-based architectures.


How to improve performance?
===========================

CPU scaling
   Viable but CFL-limited

Clock speed
   Unchanged for >15 years

Vectorization
   Potential speedup?


Vector Instructions
===================

.. image:: img/avx.svg

======   ==========  =======  ===========
Instr.   Size        GFLOP/s  Obs. (Gaea)
======   ==========  =======  ===========
SSE      2 doubles   7.2      7.196
AVX      4 doubles   14.4     14.395
======   ==========  =======  ===========

.. notes::

   Gaea C4 (Xeon E5-2697 v4, 'Broadwell")

   :math:`f_\text{peak} = 3.6 \times 10^9 \ \text{Hz}`


Fused-Multiply Add
==================

:math:`d \leftarrow a \times b + c`

======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         7.2      7.196

         FMA         14.4     14.394

AVX      Add         14.4     14.395

         FMA         28.8     28.788
======   ==========  =======  =======

:math:`\text{rd}(\text{rd}(a \times b) + c)`
vs :math:`\text{rd}(a \times b + c)`

.. notes::

   * FMAs gave a boost to certain types of computation, esp. matrix arithmetic.

   * Also offer speedy dot products

   * Note they *change answers*!  Rounding rules are technically more accurate.


Concurrency
===========

.. image:: img/broadwell_exc.svg
   :ref: https://en.wikichip.org/wiki/File:broadwell_block_diagram.svg

2 instr. per cycle, even FMA


Peak
====

======   ==========  =======  =======
Instr.   Op          GFLOP/s  Obs.
======   ==========  =======  =======
SSE      Add         7.2      7.196

         FMA         14.4     14.394

         2x FMA      28.8     28.788

AVX      Add         14.4     14.395

         Add+Mul     28.8     28.788

         FMA         28.8     28.788

         2x FMA      57.6     57.576
======   ==========  =======  =======

 
Array Performance
=================

.. image:: img/gaea_dp_flops.svg


AMD Arrays
----------

.. image:: img/zen3_dp_flops.svg

   
Single Precision
----------------

.. image:: img/gaea_flops.svg


Array Cacheing
==============

.. image:: img/gaea_dp_flops_cached.svg


AMD Cacheing
------------

.. image:: img/zen3_dp_flops_cached.svg


Memory-bound
============

.. image:: img/gaea_dp_flops_membnd.svg


Peak Array Ops
--------------

======================  ====  ====  ====  ====
Expression              Max   L2    L3    DRAM
======================  ====  ====  ====  ====
y[:] = a x[:]           11.1  3.5   1.8   0.7
y[:] = x[:] + x[:]      12.3  3.6   1.8   0.7
y[:] = x[:] + y[:]      9.3   3.5   1.8   0.7
y[:] = a x[:] + y[:]    18.3  7.0   3.6   1.4
y[:] = a x[:] + b y[:]  23.9  10.6  5.5   2.0
y[:] = x[1:] - x[:-1]   7.0   3.4   1.8   0.7
y[:] = x[8:] - x[:-8]   9.3   3.5   1.8   0.7
======================  ====  ====  ====  ====


MOM6 sample config
==================

.. image:: img/benchmark_topo.svg
   :class: float
   :width: 35%

* 32 × 32 grid, 75 level

  - ~76k / field (~:math:`2^{16}`)
  - :math:`2^{10}` per level

* 288 steps (3 day, :math:`\Delta t = 900s`)

* "Benchmark" configuration:

  - Split barotropic

  - Smagorinsky biharmonic

  - Thermo, Wright EOS

  - Bounded Coriolis terms

  - Layered (no ALE)


Profiling with perf
===================

.. image:: img/perf_symbols.png
   :width: 80%


Profiling with perf
===================

.. image:: img/perf_lines.png
   :width: 80%


perf stat
=========

.. include:: perf.stat
   :code:


Profile Overview
================

.. image:: img/subroutine_profile_gaea.svg

.. notes::

   * Speeds are on order of 1-2 GFLOP/s

   * Places most results in DRAM or L3 bound

   * EOS is doing quite well at >10 GFLOP/s


AMD Profile
-----------

.. image:: img/subroutine_profile.svg

   * btstep takes a bigger hit on AMD

   * EOS is a bit slower but still dominant

   * Speeds are a bit higher, reflecting better L2/L3 performance


Investigation Targets
=====================

* Horizontal viscosity

* Coriolis force advection (``coradcalc``)

* Barotropic solver (``btstep``)

* Vertical viscosity (``find_coupling_coef``)

* EOS (``int_density_dz_wright``)


Horizontal Viscosity
====================

.. include:: src/MOM_hor_visc.F90
   :code: fortran
   :start-line: 831
   :end-line: 958
   :number-lines: 831
   :data-line-numbers: 1|2-6|7-13|14-18|20-73|127


Non-vectorized code
===================

.. code:: x86asm

   │      │833    Shear_mag = sqrt(sh_xx(i,j)*sh_xx(i,j) + &
   │ 0.70 │         vaddsd   %xmm13,%xmm12,%xmm14
   │ 1.62 │         vsqrtsd  %xmm14,%xmm14,%xmm14
   │ 6.53 │         vmovsd   %xmm14,-0x8e8(%rbp)


==========  ========
``v___sd``  Serial
``v___pd``  Parallel
==========  ========


Excessive Stack
===============

.. code:: x86asm

   │      │919    Ah = MAX(MAX(CS%Ah_bg_xx(i,j), AhSm), AhLth)
   │      │         lea      (%rax,%rdx,8),%rdi
   │      │         lea      (%rdi,%rsi,1),%r8
   │ 0.39 │         vmovsd   (%r8,%r9,8),%xmm0
   │ 0.01 │         vmaxsd   -0x13f8(%rbp),%xmm0,%xmm0
   │ 2.34 │         vmaxsd   -0x13f0(%rbp),%xmm0,%xmm0
   │ 0.42 │         vmovsd   %xmm0,-0x1468(%rbp)

======   ===================
lea      Compute mem address
vmovsd   Serial move
vmaxsd   Serial max
======   ===================


Vectorized Horizontal Viscosity
===============================

.. include:: src/MOM_hor_visc_vec.F90
   :code: fortran
   :start-line: 854
   :end-line: 1128
   :number-lines: 831
   :data-line-numbers: 1-10|12-23|

1. Promote repeated scalars to 2d arrays

2. Move if-blocks outside loops


Hor Visc Speedup
=====================

==========  ======   ======   =======
Platform    Old      New      Speedup
==========  ======   ======   =======
Gaea C4     2.23s    1.27s    1.75x

            (1.67)   (2.93)   

AMD @ home  1.69s    1.01s    1.67x

            (2.30)   (3.76)   
==========  ======   ======   =======


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv.F90
   :code: fortran
   :start-line: 424
   :end-line: 686
   :number-lines: 424
   :data-line-numbers: 1|17-32|245-257


Coriolis advection
==================

.. include:: src/MOM_CoriolisAdv_vec.F90
   :code: fortran
   :start-line: 669
   :end-line: 694
   :number-lines: 669
   :data-line-numbers: 7-20


Coriolis Speedup
================

========    ======   ======   =======
Platform    Old      New      Speedup
========    ======   ======   =======
Gaea        1.06     0.87     1.23x

            (2.79)   (3.43)   

AMD         1.11     0.64     1.74x

            (2.68)   (5.21)   
========    ======   ======   =======


Barotropic Optimization
=======================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(1491,26)
      remark #15389: vectorization support: reference eta_wtd(i,j) has unaligned access   [ ../      ../ac/../src/core/MOM_barotropic.F90(1492,7) ]
      remark #15381: vectorization support: unaligned access used inside loop body
      remark #15305: vectorization support: vector length 2
      remark #15399: vectorization support: unroll factor set to 2
      remark #15309: vectorization support: normalized vectorization overhead 0.300
      remark #15300: LOOP WAS VECTORIZED
      remark #15451: unmasked unaligned unit stride stores: 1
      remark #15475: --- begin vector cost summary ---
      remark #15476: scalar cost: 4
      remark #15477: vector cost: 2.500
      remark #15478: estimated potential speedup: 1.450
      remark #15488: --- end vector cost summary ---
      remark #25015: Estimate of max trip count of loop=3
   LOOP END


Barotropic Optimization...?
===========================

.. code::

   LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(2383,39)
      remark #25460: No loop optimizations reported
   LOOP END

.. code::

   remark #25464: Some optimizations were skipped to constrain compile
      time. Consider overriding limits (-qoverride-limits).


Barotropic Speedup
==================

Engage the Override!

========    ======   ======   =======
Platform    Old      New      Speedup
========    ======   ======   =======
Gaea        2.35     1.60     1.46x

            (1.19)   (1.74)

AMD         3.13     1.66     1.89x

            (0.89)   (1.66)
========    ======   ======   =======

Sustainable solution is to reduce size of ``btstep()``


Vertical Viscosity
==================

.. include:: src/MOM_vert_friction.F90
   :code: fortran
   :start-line: 1248
   :end-line: 1270
   :number-lines: 1248
   :data-line-numbers: 1-23|1|23


Vertical Viscosity
==================

.. include:: src/MOM_vert_friction_vec.F90
   :code: fortran
   :start-line: 1462
   :end-line: 1489
   :number-lines: 1462
   :data-line-numbers: 1-27|23-27


Maintain Pipelines
==================

.. image:: img/stay_on_target.gif
   :width: 50%

Don't stop, keep going, even over land!


Vertical Viscosity "speedup"
============================

==================   ======   ======   ======   ======   =======
Subroutine           Old      New      Old      New      Speedup
==================   ======   ======   ======   ======   =======
vertvisc_coef        1.29     2.00     1.54     1.50     😐
find_coupling_coef   2.50     5.72     0.82     0.68     1.21x
==================   ======   ======   ======   ======   =======


Why is EOS so efficient?
========================

.. include:: src/MOM_EOS_Wright.F90
   :code: fortran
   :start-line: 578
   :end-line: 619
   :number-lines: 578

High *arithmetic intensity*: many ops per memory access


Gaea Speedup 
============

.. image:: img/subroutine_speedup_gaea.svg


AMD Speedup
-----------

.. image:: img/subroutine_speedup.svg


Other topics
============

* Index reorder (ijk -> kij)


Moving Forward
==============

Development guidelines:

* Profile and identify hotspots

* Enable vectorization

* Avoid if-blocks in loops

* Keep references close together

* Reduce memory access

* Keep functions small (<2000 lines?)


Things we haven't talked about
------------------------------

* Array alignment and peel loops

* Heap vs Stack (incl. alignment)

* Pipelining

* Arithmetic Intensity


More things we haven't talked about
-----------------------------------

Multicore/node jobs

* Clock speed throttling

* Sharing the RAM bandwidth + NUMA

* perf on MPI jobs


Whence GPUs?
============

One core vs 1 GPU process

.. image:: img/p1_gpu_flops.svg
   :width: 60%


Whence GPUs?
------------

6 cores vs 6 GPU procs

.. image:: img/p6_gpu_flops.svg
   :width: 60%


Apocrypha
=========


Ops per cycle
-------------

This is slow::

   botfn_2d(i,k) = 1.0 / (1.0 + 0.09*z2*z2*z2*z2*z2*z2)

This is faster::

   z2_sq = z2 * z2
   botfn_2d(i,k) = 1.0 / (1.0 + 0.09 * (z2_sq * z2_sq * z2_sq))

1. Fewer operations (3 vs 5 mults)

2. Fewer instructions: Two can be pipelined!


Heap vs Stack
-------------

Stack
   Local memory created by the function

Heap
   External global memory, `allocate()`

Compilers cannot manipulate heap memory, must work around ambiguity.

Stack memory can be *aligned* for better performance.


Excessive Stack?
----------------

``horizontal_viscosity`` has 18 3D arrays on stack!  And over 2x as many 2D
arrays...

.. include:: src/MOM_hor_visc_decl.F90
   :code: fortran

Moving declarations will slow model!  This is worrying...


