<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Marshall Ward">
  <meta name="dcterms.date" content="2021-05-21">
  <title>The Vectorization of MOM6</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./reveal.js/dist/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="./reveal.js/css/theme/gfdl.css" id="theme">
  <!-- Explicitly add zenburn for highlight support -->
  <link rel="stylesheet" href="./reveal.js/plugin/highlight/zenburn.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? './reveal.js/css/print/pdf.scss' : './reveal.js/css/print/paper.scss';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="./reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <base href="./index.html">
</head>
<body>
  <div class="reveal"
       style="background: url(img/bg_gfdl.jpg);
              background-size: cover;">

    <!-- Original ratio: 10.04" x 7.08" -->
    <div style="height: 100vh; position: absolute; bottom: -50vh; left: -40vh">
      <img style="height: 100vh; width: 142vh" src="img/bg_globe.png">
    </div>

    <header style="width: 10vh; position: absolute; bottom: 2vh; right: 2vh;">
      <img src="img/noaa_logo.png">
    </header>

    <footer style="font-size: 1pc; position: absolute; bottom: 2%; left: 2%;">
      <code><p><a href="https://marshallward.org/mom6vec">https://marshallward.org/mom6vec</a></p></code>
    </footer>

    <div class="slides">

<section id="title-slide">
  <!--div class="reveal" style="text-align: right;">
    <img src="img/noaa_logo.png"
         style="background: none; border: none; box-shadow: none;
         width: 30%"
         alt="NCI">
  </div-->
  <h1 class="title">The Vectorization of MOM6</h1>
  <p class="author" style="text-align: right;">Marshall Ward</p>
  <p class="date" style="text-align: right;">2021-05-21</p>
  <!-- Currently cannot add notes to a title slide, so have to do manually-->
  <aside class="notes">
    <ul>
    <li><p>In my previous role: responsible for profiling our coupled models</p>
    <ul>
    <li>especially CPU scaling</li>
    <li>Mostly MOM5, mainly ocean / sea ice</li>
    </ul></li>
    <li><p>Scaling efficiency was checked, and improvements were found</p>
    <ul>
    <li>but no real sense of how we <em>should</em> be performing</li>
    </ul></li>
    <li><p>First half will try to put performance in a bit of context</p>
    <ul>
    <li>What is "fast"? What is "fastest"?</li>
    <li>What is realistic?</li>
    <li>How should MOM6 look compared to these metrics?</li>
    </ul></li>
    <li><p>Next will go through areas we've been able to improve</p></li>
    <li><p>Final bit is open ended, I have more content to share but only if time and interest</p></li>
    <li><p>Questions are fine at any time, no urgent need to reach the end here.</p></li>
    </ul>
  </aside>
</section>

<section id="peak-performance" class="title-slide slide level1">
<h1>Peak Performance</h1>
<p><span class="math display">\[W \le N \times f \times I_\text{vec}\]</span></p>
<table>
<tbody>
<tr class="odd">
<td><span class="math inline">\(W\)</span></td>
<td>FLOPs per second ("work")</td>
</tr>
<tr class="even">
<td><span class="math inline">\(N\)</span></td>
<td># of CPUs</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(f\)</span></td>
<td>Cycles per second</td>
</tr>
<tr class="even">
<td><span class="math inline">\(I_\text{vec}\)</span></td>
<td>FLOPs per cycle</td>
</tr>
</tbody>
</table>
<aside class="notes">
<ul>
<li>"Iron law" of computing</li>
<li>Usually written as time, but inverse form is better for us.</li>
<li>"Work" here is floating point ops per second.</li>
<li>Just like real work, they are not necessarily all useful.</li>
</ul>
<p>The first two terms are very straightforward: N is the number of CPUs and f is the clock rate, or cycles per second. More CPUs and faster clock rate means more work.</p>
<p>The third term here is more interesting, and is the focus here. This represents the ability of the CPU to do concurrent work on a single core.</p>
</aside>
</section>

<section>
<section id="cpu-scaling" class="title-slide slide level1">
<h1>CPU Scaling</h1>
<p><img data-src="img/scaling_ocn.svg" style="width:75.0%" alt="image" /></p>
<p>MOM5 scaling in ACCESS-OM2</p>
<aside class="notes">
<ul>
<li><p>Figure shows a traditional scaling performance plot for MOM5 in ACCESS-OM2</p>
<ul>
<li>Focus on the left side (RHS denotes efficiency)</li>
<li>x-axis is the number of CPUs (MPI ranks)</li>
<li>y-axis is the runtime</li>
<li>Notes:
<ul>
<li>Three resolutions are shown</li>
<li>Both are logarithmic</li>
<li>Straight line denotes perfect scaling</li>
<li>Deviation denotes inefficiency</li>
</ul></li>
</ul></li>
<li><p>CPU scaling has become traditional source of improvement:</p>
<ul>
<li>And ocean models are well-suited to CPU scaling</li>
<li>But as shown, there are limits to scaling</li>
</ul></li>
<li><p>All show scaling trends, but all tapear off around 0.1s</p>
<ul>
<li>0.1Â° was limited by computer size</li>
</ul></li>
<li><p>As resolutions drop, CFL means more work per step!</p></li>
<li><p>If dt/step is capped, and steps are getting smaller, the future of CPU scaling is not so optimistic.</p></li>
</ul>
</aside>
</section>
<section class="slide level2">

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/scaling_ocn.svg" alt="image" /></p></td>
<td><p>CPU scalability has proven effective to boost performance.</p>
<p>But such methods have limits (~0.1s / step)</p>
<p>And as resolutions increase, CFL implies more steps.</p></td>
</tr>
</tbody>
</table>
</section></section>
<section>
<section id="clock-speed" class="title-slide slide level1">
<h1>Clock Speed</h1>
<p><img data-src="img/clockspeed.svg" style="width:70.0%" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>Clock speed is the former traditional source of improvement.</p></li>
<li><p>But clock speed hasn't really changed since ~2006.</p></li>
<li><p>This is from CPUDB which stops at 2014, but situation has not really changed.</p></li>
<li><p>Dennard scaling (constant power per area) presented a feasible path to improvement:</p>
<ul>
<li>Shrink area, raise f, and power consumption stays the same</li>
</ul></li>
<li><p>Until 2006 when the party ended.</p></li>
</ul>
</aside>
</section>
<section class="slide level2">

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/clockspeed.svg" alt="image" /></p></td>
<td><p>Clock speed was a traditional measure of performance</p>
<p>But overheating broke this trend around 2005 (see "Dennard scaling")</p>
<p>Since then, speeds waver around 3GHz</p></td>
</tr>
</tbody>
</table>
</section></section>
<section>
<section id="floating-point-performance" class="title-slide slide level1">
<h1>Floating Point Performance</h1>
<p><img data-src="img/specfp.svg" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>But even with static clock speeds, FP rates continued to rise.</p></li>
<li><p>This is the SPECfp benchmark, it's a bit of a mash of various FP codes, but does include some Fortran models.</p>
<ul>
<li>This is actually a composite of four SPECfp benchmarks, normalized across Intel CPUs.</li>
</ul></li>
<li><p>The source of these improvements were due in part to greater adoption of vectorization</p></li>
<li><p>Less important: What happened in the 90s?</p>
<ul>
<li>All the DEC, SPARC, etc vector CPUs. They failed to keep up with x86.</li>
</ul></li>
</ul>
</aside>
</section>
<section class="slide level2">

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/specfp.svg" alt="image" /></p></td>
<td><p>Despite halt of <span class="math inline">\(f\)</span>, floating point rates have increased</p>
<p>SPECfp shows the general FP trend over time and CPUs</p></td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><img data-src="img/specfp_intel.svg" alt="image" /></td>
<td>Usually not a good idea to combine SPECfp's, but one can normalize based on Intel CPUs to create some kind of trend.</td>
</tr>
</tbody>
</table>
</section></section>
<section id="how-to-improve-performance" class="title-slide slide level1">
<h1>How to improve performance?</h1>
<dl>
<dt>CPU scaling</dt>
<dd><p>Viable but CFL-limited</p>
</dd>
<dt>Clock speed</dt>
<dd><p>Unchanged for &gt;15 years</p>
</dd>
<dt>Vectorization</dt>
<dd><p>Potential speedup?</p>
</dd>
</dl>
<aside class="notes">
<p>So of our three sources of performance:</p>
<ul>
<li>CPU scaling seems to be getting harder</li>
<li>Clock speeds are not viable</li>
<li>FP rates are increasing, is it vectorization? Are we taking full advantage of it?</li>
</ul>
</aside>
</section>

<section id="vector-instructions" class="title-slide slide level1">
<h1>Vector Instructions</h1>
<p><img data-src="img/avx.svg" alt="image" /></p>
<table>
<thead>
<tr class="header">
<th>Instr.</th>
<th>Size</th>
<th>GFLOP/s</th>
<th>Obs. (Gaea)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SSE</td>
<td>2 doubles</td>
<td>7.2</td>
<td>7.196</td>
</tr>
<tr class="even">
<td>AVX</td>
<td>4 doubles</td>
<td>14.4</td>
<td>14.395</td>
</tr>
</tbody>
</table>
<p>(<span class="math inline">\(f_\text{peak}\)</span> = 3.6 GHz)</p>
<aside class="notes">
<ul>
<li><p>Quick review: Vectorization is an form of concurrency</p>
<ul>
<li>Combine multiple numbers into larger registers</li>
<li>Apply operations to each part of the register all at once</li>
</ul></li>
<li><p>Double precision! Not single.</p></li>
<li><p>Gaea C4 (Xeon E5-2697 v4, 'Broadwell')</p></li>
</ul>
</aside>
</section>

<section id="fused-multiply-add" class="title-slide slide level1">
<h1>Fused-Multiply Add</h1>
<p><span class="math inline">\(d \leftarrow a \times b + c\)</span></p>
<table>
<thead>
<tr class="header">
<th>Instr.</th>
<th>Op</th>
<th>GFLOP/s</th>
<th>Obs.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p>SSE</p></td>
<td><p>Add</p>
<p>FMA</p></td>
<td><p>7.2</p>
<p>14.4</p></td>
<td><p>7.196</p>
<p>14.394</p></td>
</tr>
<tr class="even">
<td><p>AVX</p></td>
<td><p>Add</p>
<p>FMA</p></td>
<td><p>14.4</p>
<p>28.8</p></td>
<td><p>14.395</p>
<p>28.788</p></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\text{rd}(\text{rd}(a \times b) + c)\)</span> vs <span class="math inline">\(\text{rd}(a \times b + c)\)</span></p>
<aside class="notes">
<ul>
<li>FMAs gave a boost to certain types of computation, esp. matrix arithmetic.</li>
<li>Also offer speedy dot products</li>
<li>Note they <em>change answers</em>! Rounding rules are technically more accurate.</li>
</ul>
</aside>
</section>

<section id="concurrency" class="title-slide slide level1">
<h1>Concurrency</h1>
<p><img data-src="img/broadwell_exc.svg" alt="image" /></p>
<p>2 inst. per cycle, even FMA</p>
<aside class="notes">
<ul>
<li><p>Modern CPUs can execute more than one instruction per cycle.</p></li>
<li><p>This is a block diagram of a Broadwell Execution Unit</p>
<ul>
<li>Main point is there are 8 ports</li>
<li>Each port can do independent tasks (if it has the data)</li>
</ul></li>
<li><p>Broadwells can do two adds, muls, or FMAs at a time</p></li>
<li><p>Non-arithmetic stuff like loop increments are usually concurrent</p></li>
</ul>
</aside>
</section>

<section>
<section id="peak-gflops" class="title-slide slide level1">
<h1>Peak GFLOPs</h1>
<table>
<thead>
<tr class="header">
<th>Operation</th>
<th>GFLOP/s</th>
<th>Obs.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Add</td>
<td>14.4</td>
<td>14.395</td>
</tr>
<tr class="even">
<td>Add+Mul</td>
<td>28.8</td>
<td>28.788</td>
</tr>
<tr class="odd">
<td>FMA</td>
<td>28.8</td>
<td>28.788</td>
</tr>
<tr class="even">
<td>2x FMA</td>
<td>57.6</td>
<td>57.576</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Putting all these concepts together:</p>
<ul>
<li>Vectorization (4 FLOP/cycle)</li>
<li>FMAs (2 FLOP/cycle)</li>
<li>Concurrency (2 FMA/cycle)</li>
</ul>
<p>We can get a 16x speedup per clock speed</p>
<p>... assuming we have enough data!</p>
</aside>
</section>
<section id="peak-sse-and-avx" class="slide level2">
<h2>Peak SSE and AVX</h2>
<table>
<thead>
<tr class="header">
<th>Instr.</th>
<th>Op</th>
<th>GFLOP/s</th>
<th>Obs.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p>SSE</p></td>
<td><p>Add</p>
<p>FMA</p>
<p>2x FMA</p></td>
<td><p>7.2</p>
<p>14.4</p>
<p>28.8</p></td>
<td><p>7.196</p>
<p>14.394</p>
<p>28.788</p></td>
</tr>
<tr class="even">
<td><p>AVX</p></td>
<td><p>Add</p>
<p>Add+Mul</p>
<p>FMA</p>
<p>2x FMA</p></td>
<td><p>14.4</p>
<p>28.8</p>
<p>28.8</p>
<p>57.6</p></td>
<td><p>14.395</p>
<p>28.788</p>
<p>28.788</p>
<p>57.576</p></td>
</tr>
</tbody>
</table>
</section></section>
<section>
<section id="array-performance" class="title-slide slide level1">
<h1>Array Performance</h1>
<p><img data-src="img/gaea_dp_flops.svg" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>How does it look in practice?</p></li>
<li><p>Loops of simple array updates are shown here.</p>
<ul>
<li>x-axis is the size of the array</li>
<li>y-axis is GFLOP/s</li>
</ul></li>
<li><p>Lots of detail here, but main points:</p>
<ul>
<li>None reach the peak (57.6) but get resonably close</li>
<li>Smaller arrays show a lot of diversity in performance</li>
<li>Large arrays drop substantially, in clear regimes</li>
<li>A lot of that diversity collapes down and is lost</li>
</ul></li>
<li><p>Other comments</p>
<ul>
<li>No assembly, this is in C and the compiler is autovectorizing.</li>
</ul></li>
</ul>
</aside>
</section>
<section id="amd-arrays" class="slide level2">
<h2>AMD Arrays</h2>
<p><img data-src="img/zen3_dp_flops.svg" alt="image" /></p>
</section>
<section id="single-precision" class="slide level2">
<h2>Single Precision</h2>
<p><img data-src="img/gaea_flops.svg" alt="image" /></p>
</section></section>
<section>
<section id="array-cacheing" class="title-slide slide level1">
<h1>Array Cacheing</h1>
<p><img data-src="img/gaea_dp_flops_cached.svg" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>Cache hierarchy explains the regimes of performance</p></li>
<li><p>Each line indicates the array length of exactly two arrays in the cache.</p></li>
<li><p>What is happening?</p>
<ul>
<li>The CPUs cannot fill the caches fast enough to complete the calculations at the optimal rate</li>
</ul></li>
<li><p>Compute-bound vs Cache-bound vs RAM-bound</p></li>
</ul>
</aside>
</section>
<section id="amd-cacheing" class="slide level2">
<h2>AMD Cacheing</h2>
<p><img data-src="img/zen3_dp_flops_cached.svg" alt="image" /></p>
</section></section>
<section id="memory-bound" class="title-slide slide level1">
<h1>Memory-bound</h1>
<p><img data-src="img/gaea_dp_flops_membnd.svg" alt="image" /></p>
<aside class="notes">
<p>More detail of the point made earlier.</p>
<p>See next slide for the actual numbers:</p>
</aside>
</section>

<section>
<section id="peak-array-ops" class="title-slide slide level1">
<h1>Peak Array Ops</h1>
<table>
<thead>
<tr class="header">
<th>Expression</th>
<th>Max</th>
<th>L2</th>
<th>L3</th>
<th>DRAM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y[:] = x[:] + y[:]</td>
<td>9.3</td>
<td>3.5</td>
<td>1.8</td>
<td>0.7</td>
</tr>
<tr class="even">
<td>y[:] = a x[:] + y[:]</td>
<td>18.3</td>
<td>7.0</td>
<td>3.6</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td>y[:] = a x[:] + b y[:]</td>
<td>23.9</td>
<td>10.6</td>
<td>5.5</td>
<td>2.0</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>What we end up with is a kind of rubric to diagnose boundedness</p>
<ul>
<li>~1 GFLOP/s: RAM-bound</li>
<li>~2-5 GFLOP/s: Cache-bound</li>
<li>&gt;10 GFLOP/s: Compute-bound</li>
</ul>
</aside>
</section>
<section id="peak-array-ops-1" class="slide level2">
<h2>Peak Array Ops</h2>
<table>
<thead>
<tr class="header">
<th>Expression</th>
<th>Max</th>
<th>L2</th>
<th>L3</th>
<th>DRAM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y[:] = a x[:]</td>
<td>11.1</td>
<td>3.5</td>
<td>1.8</td>
<td>0.7</td>
</tr>
<tr class="even">
<td>y[:] = x[:] + x[:]</td>
<td>12.3</td>
<td>3.6</td>
<td>1.8</td>
<td>0.7</td>
</tr>
<tr class="odd">
<td>y[:] = x[:] + y[:]</td>
<td>9.3</td>
<td>3.5</td>
<td>1.8</td>
<td>0.7</td>
</tr>
<tr class="even">
<td>y[:] = a x[:] + y[:]</td>
<td>18.3</td>
<td>7.0</td>
<td>3.6</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td>y[:] = a x[:] + b y[:]</td>
<td>23.9</td>
<td>10.6</td>
<td>5.5</td>
<td>2.0</td>
</tr>
<tr class="even">
<td>y[:] = x[1:] - x[:-1]</td>
<td>7.0</td>
<td>3.4</td>
<td>1.8</td>
<td>0.7</td>
</tr>
<tr class="odd">
<td>y[:] = x[8:] - x[:-8]</td>
<td>9.3</td>
<td>3.5</td>
<td>1.8</td>
<td>0.7</td>
</tr>
</tbody>
</table>
</section></section>
<section id="mom6-sample-config" class="title-slide slide level1">
<h1>MOM6 sample config</h1>
<p><img data-src="img/benchmark_topo.svg" class="float float" style="width:35.0%" alt="image" /></p>
<ul>
<li>32 Ã 32 grid, 75 level
<ul>
<li>~76k / field (~<span class="math inline">\(2^{16}\)</span>)</li>
<li><span class="math inline">\(2^{10}\)</span> per level</li>
</ul></li>
<li>288 steps (3 day, <span class="math inline">\(\Delta t = 900s\)</span>)</li>
<li>"Benchmark" configuration:
<ul>
<li>Split barotropic</li>
<li>Smagorinsky biharmonic</li>
<li>Thermo, Wright EOS</li>
<li>Bounded Coriolis terms</li>
<li>Layered (no ALE)</li>
</ul></li>
</ul>
<aside class="notes">
<ul>
<li><p>So how is MOM6 vectorization?</p>
<ul>
<li>This is a compile-time feature, so ought not depend on scaling</li>
</ul></li>
<li><p>Using a "benchmark" single-core test</p>
<ul>
<li>Reasonble set of realistic components</li>
<li>A non-trivial land/sea distribution</li>
</ul></li>
<li><p>Not necessarily the most physically signficant factors. Just the ones which had a significant impact on performance.</p></li>
</ul>
</aside>
</section>

<section id="profiling-with-perf" class="title-slide slide level1">
<h1>Profiling with perf</h1>
<p><img data-src="img/perf_symbols.png" style="width:80.0%" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>Tool of choice: perf</p></li>
<li><p>Developed for Linux kernel, distributed with it</p></li>
<li><p>Sampling profiler, low overhead, reports <em>all</em> symbols</p></li>
<li><p>Variety of metrics:</p>
<ul>
<li>Time</li>
<li>FLOPs</li>
<li>Cache hit/miss</li>
<li>Branch prediction</li>
</ul></li>
</ul>
</aside>
</section>

<section id="profiling-with-perf-1" class="title-slide slide level1">
<h1>Profiling with perf</h1>
<p><img data-src="img/perf_lines.png" style="width:80.0%" alt="image" /></p>
<aside class="notes">
<ul>
<li>Best of all, it is a line profile.</li>
<li>Bestest of all, it is an <em>assembly</em> line profiler</li>
<li>Detailed assessment of vectorization</li>
</ul>
</aside>
</section>

<section id="perf-stat" class="title-slide slide level1">
<h1>perf stat</h1>
<pre class="" data-code=""><code>Performance counter stats for &#39;../../build/intel/MOM6&#39;:

    22397.33 msec task-clock:u              # 0.986 CPUs utilized
 84092282119      cycles:u                  # 3.755 GHz
 45781959071      stalled-cycles-backend:u  # 54.44% idle cycles
 11481435369      l2_cycles_waiting_on_fills:u #  512.625 M/sec
 59207044954      fp_ret_sse_avx_ops.all:u  # 2643.487 M/sec

 22.709657811 seconds time elapsed

 22.232343000 seconds user
  0.116333000 seconds sys
</code></pre>
</section>

<section>
<section id="profile-overview" class="title-slide slide level1">
<h1>Profile Overview</h1>
<p><img data-src="img/subroutine_profile_gaea.svg" style="width:75.0%" alt="image" /></p>
<aside class="notes">
<ul>
<li><p>Hopefully not too confusing... %runtime and FLOPS together, even same axis</p></li>
<li><p>Most expensive fns are Speeds are ~1-2 GFLOP/s</p>
<ul>
<li>DRAM or L3-bound</li>
</ul></li>
<li><p>EOS is doing quite well at &gt;10 GFLOP/s</p></li>
<li><p>Also note the "long tail" in the top 50 functions.. there are no easy wins here, it will take a lot of improvements over a lot of functions to see any major speedups.</p></li>
</ul>
</aside>
</section>
<section id="amd-profile" class="slide level2">
<h2>AMD Profile</h2>
<p><img data-src="img/subroutine_profile.svg" alt="image" /></p>
<aside class="notes">
<ul>
<li>btstep takes a bigger hit on AMD</li>
<li>EOS is a bit slower but still dominant</li>
<li>Speeds are a bit higher, reflecting better L2/L3 performance</li>
</ul>
</aside>
</section></section>
<section id="investigation-targets" class="title-slide slide level1">
<h1>Investigation Targets</h1>
<ul>
<li>Horizontal viscosity</li>
<li>Coriolis force advection (<code>coradcalc</code>)</li>
<li>Barotropic solver (<code>btstep</code>)</li>
<li>Vertical viscosity (<code>find_coupling_coef</code>)</li>
<li>EOS (<code>int_density_dz_wright</code>)</li>
</ul>
<aside class="notes">
<p>Herein I will highlight our progress</p>
<ul>
<li>Investigation of some of the major bottlenecks</li>
<li>Improvements we've made</li>
</ul>
<p>But the long tail means there is much to do</p>
</aside>
</section>

<section id="horizontal-viscosity" class="title-slide slide level1">
<h1>Horizontal Viscosity</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="831

" data-end-line="958

" data-line-numbers="1|2-6|7-13|14-18|20-73|127

" data-startFrom="831

">    do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
      if ((CS%Smagorinsky_Kh) .or. (CS%Smagorinsky_Ah)) then
        Shear_mag = sqrt(sh_xx(i,j)*sh_xx(i,j) + &amp;
          0.25*((sh_xy(I-1,J-1)*sh_xy(I-1,J-1) + sh_xy(I,J)*sh_xy(I,J)) + &amp;
                (sh_xy(I-1,J)*sh_xy(I-1,J) + sh_xy(I,J-1)*sh_xy(I,J-1))))
      endif
      if ((CS%Leith_Kh) .or. (CS%Leith_Ah)) then
        if (CS%use_QG_Leith_visc) then
          vert_vort_mag = MIN(grad_vort_mag_h(i,j) + grad_div_mag_h(i,j),3.*grad_vort_mag_h_2d(i,j))
        else
          vert_vort_mag = (grad_vort_mag_h(i,j) + grad_div_mag_h(i,j))
        endif
      endif
      if (CS%better_bound_Ah .or. CS%better_bound_Kh) then
        hrat_min = min(1.0, min(h_u(I,j), h_u(I-1,j), h_v(i,J), h_v(i,J-1)) / &amp;
                            (h(i,j,k) + h_neglect) )
        visc_bound_rem = 1.0
      endif

      if (CS%Laplacian) then
        ! Determine the Laplacian viscosity at h points, using the
        ! largest value from several parameterizations.
        Kh = CS%Kh_bg_xx(i,j) ! Static (pre-computed) background viscosity
        if (CS%add_LES_viscosity) then
          if (CS%Smagorinsky_Kh) Kh = Kh + CS%Laplac2_const_xx(i,j) * Shear_mag
          if (CS%Leith_Kh) Kh = Kh + CS%Laplac3_const_xx(i,j) * vert_vort_mag*inv_PI3
        else
          if (CS%Smagorinsky_Kh) Kh = max( Kh, CS%Laplac2_const_xx(i,j) * Shear_mag )
          if (CS%Leith_Kh) Kh = max( Kh, CS%Laplac3_const_xx(i,j) * vert_vort_mag*inv_PI3)
        endif
        ! All viscosity contributions above are subject to resolution scaling
        if (rescale_Kh) Kh = VarMix%Res_fn_h(i,j) * Kh
        if (CS%res_scale_MEKE) meke_res_fn = VarMix%Res_fn_h(i,j)
        ! Older method of bounding for stability
        if (legacy_bound) Kh = min(Kh, CS%Kh_Max_xx(i,j))
        Kh = max( Kh, CS%Kh_bg_min ) ! Place a floor on the viscosity, if desired.
        if (use_MEKE_Ku) &amp;
          Kh = Kh + MEKE%Ku(i,j) * meke_res_fn ! *Add* the MEKE contribution (might be negative)
        if (CS%anisotropic) Kh = Kh + CS%Kh_aniso * ( 1. - CS%n1n2_h(i,j)**2 ) ! *Add* the tension component
                                                                               ! of anisotropic viscosity

        ! Newer method of bounding for stability
        if (CS%better_bound_Kh) then
          if (Kh &gt;= hrat_min*CS%Kh_Max_xx(i,j)) then
            visc_bound_rem = 0.0
            Kh = hrat_min*CS%Kh_Max_xx(i,j)
          else
            visc_bound_rem = 1.0 - Kh / (hrat_min*CS%Kh_Max_xx(i,j))
          endif
        endif

        if ((CS%id_Kh_h&gt;0) .or. find_FrictWork .or. CS%debug) Kh_h(i,j,k) = Kh

        if (CS%id_grid_Re_Kh&gt;0) then
          KE = 0.125*((u(I,j,k)+u(I-1,j,k))**2 + (v(i,J,k)+v(i,J-1,k))**2)
          grid_Re_Kh(i,j,k) = (sqrt(KE) * sqrt(CS%grid_sp_h2(i,j))) &amp;
              / max(Kh, CS%min_grid_Kh)
        endif

        if (CS%id_div_xx_h&gt;0) div_xx_h(i,j,k) = div_xx(i,j)
        if (CS%id_sh_xx_h&gt;0) sh_xx_h(i,j,k) = sh_xx(i,j)

        str_xx(i,j) = -Kh * sh_xx(i,j)
      else   ! not Laplacian
        str_xx(i,j) = 0.0
      endif ! Laplacian

      if (CS%anisotropic) then
        ! Shearing-strain averaged to h-points
        local_strain = 0.25 * ( (sh_xy(I,J) + sh_xy(I-1,J-1)) + (sh_xy(I-1,J) + sh_xy(I,J-1)) )
        ! *Add* the shear-strain contribution to the xx-component of stress
        str_xx(i,j) = str_xx(i,j) - CS%Kh_aniso * CS%n1n2_h(i,j) * CS%n1n1_m_n2n2_h(i,j) * local_strain
      endif

      if (CS%biharmonic) then
        ! Determine the biharmonic viscosity at h points, using the
        ! largest value from several parameterizations.
        AhSm = 0.0; AhLth = 0.0
        if ((CS%Smagorinsky_Ah) .or. (CS%Leith_Ah)) then
          if (CS%Smagorinsky_Ah) then
            if (CS%bound_Coriolis) then
              AhSm = Shear_mag * (CS%Biharm_const_xx(i,j) + &amp;
                                  CS%Biharm_const2_xx(i,j)*Shear_mag)
            else
              AhSm = CS%Biharm_const_xx(i,j) * Shear_mag
            endif
          endif
          if (CS%Leith_Ah) AhLth = CS%Biharm6_const_xx(i,j) * abs(Del2vort_h(i,j)) * inv_PI6
          Ah = MAX(MAX(CS%Ah_bg_xx(i,j), AhSm), AhLth)
          if (CS%bound_Ah .and. .not.CS%better_bound_Ah) &amp;
            Ah = MIN(Ah, CS%Ah_Max_xx(i,j))
        else
          Ah = CS%Ah_bg_xx(i,j)
        endif ! Smagorinsky_Ah or Leith_Ah

        if (use_MEKE_Au) Ah = Ah + MEKE%Au(i,j) ! *Add* the MEKE contribution

        if (CS%Re_Ah &gt; 0.0) then
          KE = 0.125*((u(I,j,k)+u(I-1,j,k))**2 + (v(i,J,k)+v(i,J-1,k))**2)
          Ah = sqrt(KE) * CS%Re_Ah_const_xx(i,j)
        endif

        if (CS%better_bound_Ah) then
          Ah = MIN(Ah, visc_bound_rem*hrat_min*CS%Ah_Max_xx(i,j))
        endif

        if ((CS%id_Ah_h&gt;0) .or. find_FrictWork .or. CS%debug) Ah_h(i,j,k) = Ah

        if (CS%id_grid_Re_Ah&gt;0) then
          KE = 0.125*((u(I,j,k)+u(I-1,j,k))**2 + (v(i,J,k)+v(i,J-1,k))**2)
          grid_Re_Ah(i,j,k) = (sqrt(KE) * CS%grid_sp_h3(i,j)) &amp;
              / max(Ah, CS%min_grid_Ah)
        endif

        str_xx(i,j) = str_xx(i,j) + Ah * &amp;
          (CS%DY_dxT(i,j) * (G%IdyCu(I,j)*Del2u(I,j) - G%IdyCu(I-1,j)*Del2u(I-1,j)) - &amp;
           CS%DX_dyT(i,j) * (G%IdxCv(i,J)*Del2v(i,J) - G%IdxCv(i,J-1)*Del2v(i,J-1)))

        ! Keep a copy of the biharmonic contribution for backscatter parameterization
        bhstr_xx(i,j) = Ah * &amp;
          (CS%DY_dxT(i,j) * (G%IdyCu(I,j)*Del2u(I,j) - G%IdyCu(I-1,j)*Del2u(I-1,j)) - &amp;
           CS%DX_dyT(i,j) * (G%IdxCv(i,J)*Del2v(i,J) - G%IdxCv(i,J-1)*Del2v(i,J-1)))
        bhstr_xx(i,j) = bhstr_xx(i,j) * (h(i,j,k) * CS%reduction_xx(i,j))

      endif  ! biharmonic

    enddo ; enddo
</code></pre>
<p>Many disparate updates per iteration</p>
<aside class="notes">
<p>If we have a look at the horizontal viscosity function:</p>
<ul>
<li>A large macro-loop doing a diverse set of tasks</li>
<li>All wrapped in if-blocks!</li>
</ul>
</aside>
</section>

<section id="non-vectorized-code" class="title-slide slide level1">
<h1>Non-vectorized code</h1>
<pre class="x86asm"><code>â      â833    Shear_mag = sqrt(sh_xx(i,j)*sh_xx(i,j) + &amp;
â 0.70 â         vaddsd   %xmm13,%xmm12,%xmm14
â 1.62 â         vsqrtsd  %xmm14,%xmm14,%xmm14
â 6.53 â         vmovsd   %xmm14,-0x8e8(%rbp)</code></pre>
<table>
<tbody>
<tr class="odd">
<td><code>v___sd</code></td>
<td>Serial</td>
</tr>
<tr class="even">
<td><code>v___pd</code></td>
<td>Parallel</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>The assembly shows that we are using vector instructions, but we are using the <em>serial form</em> of them!</p>
</aside>
</section>

<section id="excessive-stack" class="title-slide slide level1">
<h1>Excessive Stack</h1>
<pre class="x86asm"><code>â      â919    Ah = MAX(MAX(CS%Ah_bg_xx(i,j), AhSm), AhLth)
â      â         lea      (%rax,%rdx,8),%rdi
â      â         lea      (%rdi,%rsi,1),%r8
â 0.39 â         vmovsd   (%r8,%r9,8),%xmm0
â 0.01 â         vmaxsd   -0x13f8(%rbp),%xmm0,%xmm0
â 2.34 â         vmaxsd   -0x13f0(%rbp),%xmm0,%xmm0
â 0.42 â         vmovsd   %xmm0,-0x1468(%rbp)</code></pre>
<table>
<tbody>
<tr class="odd">
<td>lea</td>
<td>Compute mem address</td>
</tr>
<tr class="even">
<td>vmovsd</td>
<td>Serial move</td>
</tr>
<tr class="odd">
<td>vmaxsd</td>
<td>Serial max</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>We also see enormous amount of movement between the registers and the function's stack memory.</p>
<p>The constant "changing gears" means that</p>
<ul>
<li>the registers must be constantly</li>
<li>intermediate results need to be held in stack</li>
</ul>
</aside>
</section>

<section id="vectorized-hor-visc" class="title-slide slide level1">
<h1>Vectorized Hor Visc</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="854

" data-end-line="1128

" data-line-numbers="1-10|12-23|28-32|225-231

" data-startFrom="831

">    if ((CS%Smagorinsky_Kh) .or. (CS%Smagorinsky_Ah)) then
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        sh_xx_sq = sh_xx(i,j) * sh_xx(i,j)
        sh_xy_sq = 0.25 * ( &amp;
            (sh_xy(I-1,J-1) * sh_xy(I-1,J-1) + sh_xy(I,J) * sh_xy(I,J)) &amp;
            + (sh_xy(I-1,J) * sh_xy(I-1,J) + sh_xy(I,J-1) * sh_xy(I,J-1)) &amp;
        )
        Shear_mag(i,j) = sqrt(sh_xx_sq + sh_xy_sq)
      enddo ; enddo
    endif

    if (CS%better_bound_Ah .or. CS%better_bound_Kh) then
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        h_min = min(h_u(I,j), h_u(I-1,j), h_v(i,J), h_v(i,J-1))
        hrat_min(i,j) = min(1.0, h_min / (h(i,j,k) + h_neglect))
      enddo ; enddo

      if (CS%better_bound_Kh) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          visc_bound_rem(i,j) = 1.0
        enddo ; enddo
      endif
    endif

    if (CS%Laplacian) then
      if ((CS%Leith_Kh) .or. (CS%Leith_Ah)) then
        if (CS%use_QG_Leith_visc) then
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            grad_vort = grad_vort_mag_h(i,j) + grad_div_mag_h(i,j)
            grad_vort_qg = 3. * grad_vort_mag_h_2d(i,j)
            vert_vort_mag(i,j) = min(grad_vort, grad_vort_qg)
          enddo ; enddo
        else
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            vert_vort_mag(i,j) = grad_vort_mag_h(i,j) + grad_div_mag_h(i,j)
          enddo ; enddo
        endif
      endif

      ! Determine the Laplacian viscosity at h points, using the
      ! largest value from several parameterizations.

      ! Static (pre-computed) background viscosity
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        Kh(i,j) = CS%Kh_bg_xx(i,j)
      enddo ; enddo

      ! NOTE: The following do-block can be decomposed and vectorized after the
      !   stack size has been reduced.
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        if (CS%add_LES_viscosity) then
          if (CS%Smagorinsky_Kh) &amp;
            Kh(i,j) = Kh(i,j) + CS%Laplac2_const_xx(i,j) * Shear_mag(i,j)
          if (CS%Leith_Kh) &amp;
            Kh(i,j) = Kh(i,j) + CS%Laplac3_const_xx(i,j) * vert_vort_mag(i,j) * inv_PI3
        else
          if (CS%Smagorinsky_Kh) &amp;
            Kh(i,j) = max(Kh(i,j), CS%Laplac2_const_xx(i,j) * Shear_mag(i,j))
          if (CS%Leith_Kh) &amp;
            Kh(i,j) = max(Kh(i,j), CS%Laplac3_const_xx(i,j) * vert_vort_mag(i,j) * inv_PI3)
        endif
      enddo ; enddo

      ! All viscosity contributions above are subject to resolution scaling

      if (rescale_Kh) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          Kh(i,j) = VarMix%Res_fn_h(i,j) * Kh(i,j)
        enddo ; enddo
      endif

      if (legacy_bound) then
        ! Older method of bounding for stability
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          Kh(i,j) = min(Kh(i,j), CS%Kh_Max_xx(i,j))
        enddo ; enddo
      endif

      ! Place a floor on the viscosity, if desired.
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        Kh(i,j) = max(Kh(i,j), CS%Kh_bg_min)
      enddo ; enddo

      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        if (CS%res_scale_MEKE) meke_res_fn = VarMix%Res_fn_h(i,j)

        if (use_MEKE_Ku) &amp;
          ! *Add* the MEKE contribution (might be negative)
          Kh(i,j) = Kh(i,j) + MEKE%Ku(i,j) * meke_res_fn
      enddo ; enddo

      if (CS%anisotropic) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          ! *Add* the tension component of anisotropic viscosity
          Kh(i,j) = Kh(i,j) + CS%Kh_aniso * (1. - CS%n1n2_h(i,j)**2)
        enddo ; enddo
      endif

      ! Newer method of bounding for stability
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        if (CS%better_bound_Kh) then
          if (Kh(i,j) &gt;= hrat_min(i,j) * CS%Kh_Max_xx(i,j)) then
            visc_bound_rem(i,j) = 0.0
            Kh(i,j) = hrat_min(i,j) * CS%Kh_Max_xx(i,j)
          else
            visc_bound_rem(i,j) = 1.0 - Kh(i,j) / (hrat_min(i,j) * CS%Kh_Max_xx(i,j))
          endif
        endif
      enddo ; enddo

      if (CS%id_Kh_h&gt;0 .or. CS%debug) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          Kh_h(i,j,k) = Kh(i,j)
        enddo ; enddo
      endif

      if (CS%id_grid_Re_Kh&gt;0) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          KE = 0.125*((u(I,j,k)+u(I-1,j,k))**2 + (v(i,J,k)+v(i,J-1,k))**2)
          grid_Kh = max(Kh(i,j), CS%min_grid_Kh)
          grid_Re_Kh(i,j,k) = (sqrt(KE) * sqrt(CS%grid_sp_h2(i,j))) / grid_Kh
        enddo ; enddo
      endif

      if (CS%id_div_xx_h&gt;0) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          div_xx_h(i,j,k) = div_xx(i,j)
        enddo ; enddo
      endif

      if (CS%id_sh_xx_h&gt;0) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          sh_xx_h(i,j,k) = sh_xx(i,j)
        enddo ; enddo
      endif

      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        str_xx(i,j) = -Kh(i,j) * sh_xx(i,j)
      enddo ; enddo
    else
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        str_xx(i,j) = 0.0
      enddo ; enddo
    endif

    if (CS%anisotropic) then
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        ! Shearing-strain averaged to h-points
        local_strain = 0.25 * ( (sh_xy(I,J) + sh_xy(I-1,J-1)) + (sh_xy(I-1,J) + sh_xy(I,J-1)) )
        ! *Add* the shear-strain contribution to the xx-component of stress
        str_xx(i,j) = str_xx(i,j) - CS%Kh_aniso * CS%n1n2_h(i,j) * CS%n1n1_m_n2n2_h(i,j) * local_strain
      enddo ; enddo
    endif

    if (CS%biharmonic) then
      ! Determine the biharmonic viscosity at h points, using the
      ! largest value from several parameterizations.
      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        Ah(i,j) = CS%Ah_bg_xx(i,j)
      enddo ; enddo

      if ((CS%Smagorinsky_Ah) .or. (CS%Leith_Ah)) then
        if (CS%Smagorinsky_Ah) then
          if (CS%bound_Coriolis) then
            do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
              AhSm = Shear_mag(i,j) * (CS%Biharm_const_xx(i,j) &amp;
                  + CS%Biharm_const2_xx(i,j) * Shear_mag(i,j) &amp;
              )
              Ah(i,j) = max(Ah(i,j), AhSm)
            enddo ; enddo
          else
            do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
              AhSm = CS%Biharm_const_xx(i,j) * Shear_mag(i,j)
              Ah(i,j) = max(Ah(i,j), AhSm)
            enddo ; enddo
          endif
        endif

        if (CS%Leith_Ah) then
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            AhLth = CS%Biharm6_const_xx(i,j) * abs(Del2vort_h(i,j)) * inv_PI6
            Ah(i,j) = max(Ah(i,j), AhLth)
          enddo ; enddo
        endif

        if (CS%bound_Ah .and. .not. CS%better_bound_Ah) then
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            Ah(i,j) = min(Ah(i,j), CS%Ah_Max_xx(i,j))
          enddo ; enddo
        endif
      endif ! Smagorinsky_Ah or Leith_Ah

      if (use_MEKE_Au) then
        ! *Add* the MEKE contribution
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          Ah(i,j) = Ah(i,j) + MEKE%Au(i,j)
        enddo ; enddo
      endif

      if (CS%Re_Ah &gt; 0.0) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          KE = 0.125*((u(I,j,k)+u(I-1,j,k))**2 + (v(i,J,k)+v(i,J-1,k))**2)
          Ah(i,j) = sqrt(KE) * CS%Re_Ah_const_xx(i,j)
        enddo ; enddo
      endif

      if (CS%better_bound_Ah) then
        if (CS%better_bound_Kh) then
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            Ah(i,j) = min(Ah(i,j), visc_bound_rem(i,j) * hrat_min(i,j) * CS%Ah_Max_xx(i,j))
          enddo ; enddo
        else
          do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
            Ah(i,j) = min(Ah(i,j), hrat_min(i,j) * CS%Ah_Max_xx(i,j))
          enddo ; enddo
        endif
      endif

      if ((CS%id_Ah_h&gt;0) .or. CS%debug) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          Ah_h(i,j,k) = Ah(i,j)
        enddo ; enddo
      endif

      if (CS%id_grid_Re_Ah&gt;0) then
        do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
          KE = 0.125 * ((u(I,j,k) + u(I-1,j,k))**2 + (v(i,J,k) + v(i,J-1,k))**2)
          grid_Ah = max(Ah(i,j), CS%min_grid_Ah)
          grid_Re_Ah(i,j,k) = (sqrt(KE) * CS%grid_sp_h3(i,j)) / grid_Ah
        enddo ; enddo
      endif

      do j=Jsq,Jeq+1 ; do i=Isq,Ieq+1
        d_del2u = G%IdyCu(I,j) * Del2u(I,j) - G%IdyCu(I-1,j) * Del2u(I-1,j)
        d_del2v = G%IdxCv(i,J) * Del2v(i,J) - G%IdxCv(i,J-1) * Del2v(i,J-1)
        d_str = Ah(i,j) * (CS%DY_dxT(i,j) * d_del2u - CS%DX_dyT(i,j) * d_del2v)

        str_xx(i,j) = str_xx(i,j) + d_str

        ! Keep a copy of the biharmonic contribution for backscatter parameterization
        bhstr_xx(i,j) = d_str * (h(i,j,k) * CS%reduction_xx(i,j))
      enddo ; enddo
    endif

    if (CS%biharmonic) then
      ! Gradient of Laplacian, for use in bi-harmonic term
      do J=js-1,Jeq ; do I=is-1,Ieq
        dDel2vdx(I,J) = CS%DY_dxBu(I,J)*(Del2v(i+1,J)*G%IdyCv(i+1,J) - Del2v(i,J)*G%IdyCv(i,J))
        dDel2udy(I,J) = CS%DX_dyBu(I,J)*(Del2u(I,j+1)*G%IdxCu(I,j+1) - Del2u(I,j)*G%IdxCu(I,j))
      enddo ; enddo
      ! Adjust contributions to shearing strain on open boundaries.
      if (apply_OBC) then ; if (OBC%zero_strain .or. OBC%freeslip_strain) then
        do n=1,OBC%number_of_segments
          J = OBC%segment(n)%HI%JsdB ; I = OBC%segment(n)%HI%IsdB
          if (OBC%segment(n)%is_N_or_S .and. (J &gt;= js-1) .and. (J &lt;= Jeq)) then
            do I=OBC%segment(n)%HI%IsdB,OBC%segment(n)%HI%IedB
              if (OBC%zero_strain) then
                dDel2vdx(I,J) = 0. ; dDel2udy(I,J) = 0.
              elseif (OBC%freeslip_strain) then
                dDel2udy(I,J) = 0.
              endif
            enddo
          elseif (OBC%segment(n)%is_E_or_W .and. (I &gt;= is-1) .and. (I &lt;= Ieq)) then
            do J=OBC%segment(n)%HI%JsdB,OBC%segment(n)%HI%JedB
              if (OBC%zero_strain) then
                dDel2vdx(I,J) = 0. ; dDel2udy(I,J) = 0.
              elseif (OBC%freeslip_strain) then
                dDel2vdx(I,J) = 0.
              endif
            enddo
          endif
        enddo
      endif ; endif
    endif
</code></pre>
<ol type="1">
<li>Move if-blocks outside loops</li>
<li>Promote scalars to 2d arrays (sparingly)</li>
</ol>
<aside class="notes">
<p>How to we resolve this?</p>
<ul>
<li><p>Break the large loop into many smaller loops</p></li>
<li><p>Promote scalars to 2d arrays when needed</p>
<blockquote>
<p>(Slightly unintuitive, but it is a win here)</p>
</blockquote></li>
</ul>
</aside>
</section>

<section id="hor-visc-speedup" class="title-slide slide level1">
<h1>Hor Visc Speedup</h1>
<table>
<thead>
<tr class="header">
<th>Platform</th>
<th>Runtime</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intel (Gaea)</td>
<td>2.23s â 1.27s</td>
<td>1.75x</td>
</tr>
<tr class="even">
<td>AMD (home)</td>
<td>1.69s â 1.01s</td>
<td>1.67x</td>
</tr>
</tbody>
</table>
</section>

<section id="coriolis-advection" class="title-slide slide level1">
<h1>Coriolis advection</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="424

" data-end-line="686

" data-line-numbers="1|17-32|245-257

" data-startFrom="424

">    do J=Jsq-1,Jeq+1 ; do I=Isq-1,Ieq+1
      if (CS%no_slip ) then
        relative_vorticity = (2.0-G%mask2dBu(I,J)) * (dvdx(I,J) - dudy(I,J)) * G%IareaBu(I,J)
      else
        relative_vorticity = G%mask2dBu(I,J) * (dvdx(I,J) - dudy(I,J)) * G%IareaBu(I,J)
      endif
      absolute_vorticity = G%CoriolisBu(I,J) + relative_vorticity
      Ih = 0.0
      if (Area_q(i,j) &gt; 0.0) then
        hArea_q = (hArea_u(I,j) + hArea_u(I,j+1)) + (hArea_v(i,J) + hArea_v(i+1,J))
        Ih = Area_q(i,j) / (hArea_q + h_neglect*Area_q(i,j))
      endif
      q(I,J) = absolute_vorticity * Ih
      abs_vort(I,J) = absolute_vorticity
      Ih_q(I,J) = Ih

      if (CS%bound_Coriolis) then
        fv1 = absolute_vorticity * v(i+1,J,k)
        fv2 = absolute_vorticity * v(i,J,k)
        fu1 = -absolute_vorticity * u(I,j+1,k)
        fu2 = -absolute_vorticity * u(I,j,k)
        if (fv1 &gt; fv2) then
          max_fvq(I,J) = fv1 ; min_fvq(I,J) = fv2
        else
          max_fvq(I,J) = fv2 ; min_fvq(I,J) = fv1
        endif
        if (fu1 &gt; fu2) then
          max_fuq(I,J) = fu1 ; min_fuq(I,J) = fu2
        else
          max_fuq(I,J) = fu2 ; min_fuq(I,J) = fu1
        endif
      endif

      if (CS%id_rv &gt; 0) RV(I,J,k) = relative_vorticity
      if (CS%id_PV &gt; 0) PV(I,J,k) = q(I,J)
      if (associated(AD%rv_x_v) .or. associated(AD%rv_x_u)) &amp;
        q2(I,J) = relative_vorticity * Ih
    enddo ; enddo

    !   a, b, c, and d are combinations of neighboring potential
    ! vorticities which form the Arakawa and Hsu vorticity advection
    ! scheme.  All are defined at u grid points.

    if (CS%Coriolis_Scheme == ARAKAWA_HSU90) then
      do j=Jsq,Jeq+1
        do I=is-1,Ieq
          a(I,j) = (q(I,J) + (q(I+1,J) + q(I,J-1))) * C1_12
          d(I,j) = ((q(I,J) + q(I+1,J-1)) + q(I,J-1)) * C1_12
        enddo
        do I=Isq,Ieq
          b(I,j) = (q(I,J) + (q(I-1,J) + q(I,J-1))) * C1_12
          c(I,j) = ((q(I,J) + q(I-1,J-1)) + q(I,J-1)) * C1_12
        enddo
      enddo
    elseif (CS%Coriolis_Scheme == ARAKAWA_LAMB81) then
      do j=Jsq,Jeq+1 ; do I=Isq,Ieq+1
        a(I-1,j) = (2.0*(q(I,J) + q(I-1,J-1)) + (q(I-1,J) + q(I,J-1))) * C1_24
        d(I-1,j) = ((q(I,j) + q(I-1,J-1)) + 2.0*(q(I-1,J) + q(I,J-1))) * C1_24
        b(I,j) =   ((q(I,J) + q(I-1,J-1)) + 2.0*(q(I-1,J) + q(I,J-1))) * C1_24
        c(I,j) =   (2.0*(q(I,J) + q(I-1,J-1)) + (q(I-1,J) + q(I,J-1))) * C1_24
        ep_u(i,j) = ((q(I,J) - q(I-1,J-1)) + (q(I-1,J) - q(I,J-1))) * C1_24
        ep_v(i,j) = (-(q(I,J) - q(I-1,J-1)) + (q(I-1,J) - q(I,J-1))) * C1_24
      enddo ; enddo
    elseif (CS%Coriolis_Scheme == AL_BLEND) then
      Fe_m2 = CS%F_eff_max_blend - 2.0
      rat_lin = 1.5 * Fe_m2 / max(CS%wt_lin_blend, 1.0e-16)

      ! This allows the code to always give Sadourny Energy
      if (CS%F_eff_max_blend &lt;= 2.0) then ; Fe_m2 = -1. ; rat_lin = -1.0 ; endif

      do j=Jsq,Jeq+1 ; do I=Isq,Ieq+1
        min_Ihq = MIN(Ih_q(I-1,J-1), Ih_q(I,J-1), Ih_q(I-1,J), Ih_q(I,J))
        max_Ihq = MAX(Ih_q(I-1,J-1), Ih_q(I,J-1), Ih_q(I-1,J), Ih_q(I,J))
        rat_m1 = 1.0e15
        if (max_Ihq &lt; 1.0e15*min_Ihq) rat_m1 = max_Ihq / min_Ihq - 1.0
        ! The weights used here are designed to keep the effective Coriolis
        ! acceleration from any one point on its neighbors within a factor
        ! of F_eff_max.  The minimum permitted value is 2 (the factor for
        ! Sadourny&#39;s energy conserving scheme).

        ! Determine the relative weights of Arakawa &amp; Lamb vs. Arakawa and Hsu.
        if (rat_m1 &lt;= Fe_m2) then ; AL_wt = 1.0
        elseif (rat_m1 &lt; 1.5*Fe_m2) then ; AL_wt = 3.0*Fe_m2 / rat_m1 - 2.0
        else ; AL_wt = 0.0 ; endif

        ! Determine the relative weights of Sadourny Energy vs. the other two.
        if (rat_m1 &lt;= 1.5*Fe_m2) then ; Sad_wt = 0.0
        elseif (rat_m1 &lt;= rat_lin) then
          Sad_wt = 1.0 - (1.5*Fe_m2) / rat_m1
        elseif (rat_m1 &lt; 2.0*rat_lin) then
          Sad_wt = 1.0 - (CS%wt_lin_blend / rat_lin) * (rat_m1 - 2.0*rat_lin)
        else ; Sad_wt = 1.0 ; endif

        a(I-1,j) = Sad_wt * 0.25 * q(I-1,J) + (1.0 - Sad_wt) * &amp;
                   ( ((2.0-AL_wt)* q(I-1,J) + AL_wt*q(I,J-1)) + &amp;
                      2.0 * (q(I,J) + q(I-1,J-1)) ) * C1_24
        d(I-1,j) = Sad_wt * 0.25 * q(I-1,J-1) + (1.0 - Sad_wt) * &amp;
                   ( ((2.0-AL_wt)* q(I-1,J-1) + AL_wt*q(I,J)) + &amp;
                      2.0 * (q(I-1,J) + q(I,J-1)) ) * C1_24
        b(I,j) =   Sad_wt * 0.25 * q(I,J) + (1.0 - Sad_wt) * &amp;
                   ( ((2.0-AL_wt)* q(I,J) + AL_wt*q(I-1,J-1)) + &amp;
                      2.0 * (q(I-1,J) + q(I,J-1)) ) * C1_24
        c(I,j) =   Sad_wt * 0.25 * q(I,J-1) + (1.0 - Sad_wt) * &amp;
                   ( ((2.0-AL_wt)* q(I,J-1) + AL_wt*q(I-1,J)) + &amp;
                      2.0 * (q(I,J) + q(I-1,J-1)) ) * C1_24
        ep_u(i,j) = AL_wt  * ((q(I,J) - q(I-1,J-1)) + (q(I-1,J) - q(I,J-1))) * C1_24
        ep_v(i,j) = AL_wt * (-(q(I,J) - q(I-1,J-1)) + (q(I-1,J) - q(I,J-1))) * C1_24
      enddo ; enddo
    endif

    if (CS%Coriolis_En_Dis) then
    !  c1 = 1.0-1.5*RANGE ; c2 = 1.0-RANGE ; c3 = 2.0 ; slope = 0.5
      c1 = 1.0-1.5*0.5 ; c2 = 1.0-0.5 ; c3 = 2.0 ; slope = 0.5

      do j=Jsq,Jeq+1 ; do I=is-1,ie
        uhc = uh_center(I,j)
        uhm = uh(I,j,k)
        ! This sometimes matters with some types of open boundary conditions.
        if (G%dy_Cu(I,j) == 0.0) uhc = uhm

        if (abs(uhc) &lt; 0.1*abs(uhm)) then
          uhm = 10.0*uhc
        elseif (abs(uhc) &gt; c1*abs(uhm)) then
          if (abs(uhc) &lt; c2*abs(uhm)) then ; uhc = (3.0*uhc+(1.0-c2*3.0)*uhm)
          elseif (abs(uhc) &lt;= c3*abs(uhm)) then ; uhc = uhm
          else ; uhc = slope*uhc+(1.0-c3*slope)*uhm
          endif
        endif

        if (uhc &gt; uhm) then
          uh_min(I,j) = uhm ; uh_max(I,j) = uhc
        else
          uh_max(I,j) = uhm ; uh_min(I,j) = uhc
        endif
      enddo ; enddo
      do J=js-1,je ; do i=Isq,Ieq+1
        vhc = vh_center(i,J)
        vhm = vh(i,J,k)
        ! This sometimes matters with some types of open boundary conditions.
        if (G%dx_Cv(i,J) == 0.0) vhc = vhm

        if (abs(vhc) &lt; 0.1*abs(vhm)) then
          vhm = 10.0*vhc
        elseif (abs(vhc) &gt; c1*abs(vhm)) then
          if (abs(vhc) &lt; c2*abs(vhm)) then ; vhc = (3.0*vhc+(1.0-c2*3.0)*vhm)
          elseif (abs(vhc) &lt;= c3*abs(vhm)) then ; vhc = vhm
          else ; vhc = slope*vhc+(1.0-c3*slope)*vhm
          endif
        endif

        if (vhc &gt; vhm) then
          vh_min(i,J) = vhm ; vh_max(i,J) = vhc
        else
          vh_max(i,J) = vhm ; vh_min(i,J) = vhc
        endif
      enddo ; enddo
    endif

    ! Calculate KE and the gradient of KE
    call gradKE(u, v, h, KE, KEx, KEy, k, OBC, G, GV, US, CS)

    ! Calculate the tendencies of zonal velocity due to the Coriolis
    ! force and momentum advection.  On a Cartesian grid, this is
    !     CAu =  q * vh - d(KE)/dx.
    if (CS%Coriolis_Scheme == SADOURNY75_ENERGY) then
      if (CS%Coriolis_En_Dis) then
        ! Energy dissipating biased scheme, Hallberg 200x
        do j=js,je ; do I=Isq,Ieq
          if (q(I,J)*u(I,j,k) == 0.0) then
            temp1 = q(I,J) * ( (vh_max(i,j)+vh_max(i+1,j)) &amp;
                             + (vh_min(i,j)+vh_min(i+1,j)) )*0.5
          elseif (q(I,J)*u(I,j,k) &lt; 0.0) then
            temp1 = q(I,J) * (vh_max(i,j)+vh_max(i+1,j))
          else
            temp1 = q(I,J) * (vh_min(i,j)+vh_min(i+1,j))
          endif
          if (q(I,J-1)*u(I,j,k) == 0.0) then
            temp2 = q(I,J-1) * ( (vh_max(i,j-1)+vh_max(i+1,j-1)) &amp;
                               + (vh_min(i,j-1)+vh_min(i+1,j-1)) )*0.5
          elseif (q(I,J-1)*u(I,j,k) &lt; 0.0) then
            temp2 = q(I,J-1) * (vh_max(i,j-1)+vh_max(i+1,j-1))
          else
            temp2 = q(I,J-1) * (vh_min(i,j-1)+vh_min(i+1,j-1))
          endif
          CAu(I,j,k) = 0.25 * G%IdxCu(I,j) * (temp1 + temp2)
        enddo ; enddo
      else
        ! Energy conserving scheme, Sadourny 1975
        do j=js,je ; do I=Isq,Ieq
          CAu(I,j,k) = 0.25 * &amp;
            (q(I,J) * (vh(i+1,J,k) + vh(i,J,k)) + &amp;
             q(I,J-1) * (vh(i,J-1,k) + vh(i+1,J-1,k))) * G%IdxCu(I,j)
        enddo ; enddo
      endif
    elseif (CS%Coriolis_Scheme == SADOURNY75_ENSTRO) then
      do j=js,je ; do I=Isq,Ieq
        CAu(I,j,k) = 0.125 * (G%IdxCu(I,j) * (q(I,J) + q(I,J-1))) * &amp;
                     ((vh(i+1,J,k) + vh(i,J,k)) + (vh(i,J-1,k) + vh(i+1,J-1,k)))
      enddo ; enddo
    elseif ((CS%Coriolis_Scheme == ARAKAWA_HSU90) .or. &amp;
            (CS%Coriolis_Scheme == ARAKAWA_LAMB81) .or. &amp;
            (CS%Coriolis_Scheme == AL_BLEND)) then
      ! (Global) Energy and (Local) Enstrophy conserving, Arakawa &amp; Hsu 1990
      do j=js,je ; do I=Isq,Ieq
        CAu(I,j,k) = ((a(I,j) * vh(i+1,J,k) +  c(I,j) * vh(i,J-1,k))  + &amp;
                      (b(I,j) * vh(i,J,k) +  d(I,j) * vh(i+1,J-1,k))) * G%IdxCu(I,j)
      enddo ; enddo
    elseif (CS%Coriolis_Scheme == ROBUST_ENSTRO) then
      ! An enstrophy conserving scheme robust to vanishing layers
      ! Note: Heffs are in lieu of h_at_v that should be returned by the
      !       continuity solver. AJA
      do j=js,je ; do I=Isq,Ieq
        Heff1 = abs(vh(i,J,k) * G%IdxCv(i,J)) / (eps_vel+abs(v(i,J,k)))
        Heff1 = max(Heff1, min(h(i,j,k),h(i,j+1,k)))
        Heff1 = min(Heff1, max(h(i,j,k),h(i,j+1,k)))
        Heff2 = abs(vh(i,J-1,k) * G%IdxCv(i,J-1)) / (eps_vel+abs(v(i,J-1,k)))
        Heff2 = max(Heff2, min(h(i,j-1,k),h(i,j,k)))
        Heff2 = min(Heff2, max(h(i,j-1,k),h(i,j,k)))
        Heff3 = abs(vh(i+1,J,k) * G%IdxCv(i+1,J)) / (eps_vel+abs(v(i+1,J,k)))
        Heff3 = max(Heff3, min(h(i+1,j,k),h(i+1,j+1,k)))
        Heff3 = min(Heff3, max(h(i+1,j,k),h(i+1,j+1,k)))
        Heff4 = abs(vh(i+1,J-1,k) * G%IdxCv(i+1,J-1)) / (eps_vel+abs(v(i+1,J-1,k)))
        Heff4 = max(Heff4, min(h(i+1,j-1,k),h(i+1,j,k)))
        Heff4 = min(Heff4, max(h(i+1,j-1,k),h(i+1,j,k)))
        if (CS%PV_Adv_Scheme == PV_ADV_CENTERED) then
          CAu(I,j,k) = 0.5*(abs_vort(I,J)+abs_vort(I,J-1)) * &amp;
                       ((vh(i,J,k) + vh(i+1,J-1,k)) + (vh(i,J-1,k) + vh(i+1,J,k)) ) /  &amp;
                       (h_tiny + ((Heff1+Heff4) + (Heff2+Heff3)) ) * G%IdxCu(I,j)
        elseif (CS%PV_Adv_Scheme == PV_ADV_UPWIND1) then
          VHeff = ((vh(i,J,k) + vh(i+1,J-1,k)) + (vh(i,J-1,k) + vh(i+1,J,k)) )
          QVHeff = 0.5*( (abs_vort(I,J)+abs_vort(I,J-1))*VHeff &amp;
                        -(abs_vort(I,J)-abs_vort(I,J-1))*abs(VHeff) )
          CAu(I,j,k) = (QVHeff / ( h_tiny + ((Heff1+Heff4) + (Heff2+Heff3)) ) ) * G%IdxCu(I,j)
        endif
      enddo ; enddo
    endif
    ! Add in the additonal terms with Arakawa &amp; Lamb.
    if ((CS%Coriolis_Scheme == ARAKAWA_LAMB81) .or. &amp;
        (CS%Coriolis_Scheme == AL_BLEND)) then ; do j=js,je ; do I=Isq,Ieq
      CAu(I,j,k) = CAu(I,j,k) + &amp;
            (ep_u(i,j)*uh(I-1,j,k) - ep_u(i+1,j)*uh(I+1,j,k)) * G%IdxCu(I,j)
    enddo ; enddo ; endif


    if (CS%bound_Coriolis) then
      do j=js,je ; do I=Isq,Ieq
        max_fv = MAX(max_fvq(I,J), max_fvq(I,J-1))
        min_fv = MIN(min_fvq(I,J), min_fvq(I,J-1))
       ! CAu(I,j,k) = min( CAu(I,j,k), max_fv )
       ! CAu(I,j,k) = max( CAu(I,j,k), min_fv )
        if (CAu(I,j,k) &gt; max_fv) then
            CAu(I,j,k) = max_fv
        else
          if (CAu(I,j,k) &lt; min_fv) CAu(I,j,k) = min_fv
        endif
      enddo ; enddo
    endif

    ! Term - d(KE)/dx.
    do j=js,je ; do I=Isq,Ieq
      CAu(I,j,k) = CAu(I,j,k) - KEx(I,j)
      if (associated(AD%gradKEu)) AD%gradKEu(I,j,k) = -KEx(I,j)
</code></pre>
<p>200 lines separate <code>max_fvq</code> write and read</p>
</section>

<section id="coriolis-advection-1" class="title-slide slide level1">
<h1>Coriolis advection</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="669

" data-end-line="694

" data-line-numbers="7-20

" data-startFrom="669

">    ! Add in the additonal terms with Arakawa &amp; Lamb.
    if ((CS%Coriolis_Scheme == ARAKAWA_LAMB81) .or. &amp;
        (CS%Coriolis_Scheme == AL_BLEND)) then ; do j=js,je ; do I=Isq,Ieq
      CAu(I,j,k) = CAu(I,j,k) + &amp;
            (ep_u(i,j)*uh(I-1,j,k) - ep_u(i+1,j)*uh(I+1,j,k)) * G%IdxCu(I,j)
    enddo ; enddo ; endif

    if (CS%bound_Coriolis) then
      do j=js,je ; do I=Isq,Ieq
        fv1 = abs_vort(I,J) * v(i+1,J,k)
        fv2 = abs_vort(I,J) * v(i,J,k)
        fv3 = abs_vort(I,J-1) * v(i+1,J-1,k)
        fv4 = abs_vort(I,J-1) * v(i,J-1,k)

        max_fv = max(fv1, fv2, fv3, fv4)
        min_fv = min(fv1, fv2, fv3, fv4)

        CAu(I,j,k) = min(CAu(I,j,k), max_fv)
        CAu(I,j,k) = max(CAu(I,j,k), min_fv)
      enddo ; enddo
    endif

    ! Term - d(KE)/dx.
    do j=js,je ; do I=Isq,Ieq
      CAu(I,j,k) = CAu(I,j,k) - KEx(I,j)
</code></pre>
<p>Keep fields collocated</p>
</section>

<section id="coriolis-speedup" class="title-slide slide level1">
<h1>Coriolis Speedup</h1>
<table>
<thead>
<tr class="header">
<th>Platform</th>
<th>Runtime</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intel</td>
<td>1.06s â 0.87s</td>
<td>1.23x</td>
</tr>
<tr class="even">
<td>AMD</td>
<td>1.11s â 0.64s</td>
<td>1.74x</td>
</tr>
</tbody>
</table>
</section>

<section id="barotropic-optimization" class="title-slide slide level1">
<h1>Barotropic Optimization</h1>
<pre class=""><code>LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(1491,26)
   remark #15389: vectorization support: reference eta_wtd(i,j) has unaligned access   [ ../../ac/../src/core/MOM_barotropic.F90(1492,7) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15305: vectorization support: vector length 2
   remark #15399: vectorization support: unroll factor set to 2
   remark #15309: vectorization support: normalized vectorization overhead 0.300
   remark #15300: LOOP WAS VECTORIZED
   remark #15451: unmasked unaligned unit stride stores: 1
   remark #15475: --- begin vector cost summary ---
   remark #15476: scalar cost: 4
   remark #15477: vector cost: 2.500
   remark #15478: estimated potential speedup: 1.450
   remark #15488: --- end vector cost summary ---
   remark #25015: Estimate of max trip count of loop=3
LOOP END</code></pre>
<p>A typical vectorization report</p>
</section>

<section id="barotropic-optimization..." class="title-slide slide level1">
<h1>Barotropic Optimization...?</h1>
<pre class=""><code>LOOP BEGIN at ../../ac/../src/core/MOM_barotropic.F90(2383,39)
   remark #25460: No loop optimizations reported
LOOP END</code></pre>
<pre class=""><code>remark #25464: Some optimizations were skipped to constrain compile
   time. Consider overriding limits (-qoverride-limits).</code></pre>
<p>2000 lines may be too long...</p>
</section>

<section id="barotropic-speedup" class="title-slide slide level1">
<h1>Barotropic Speedup</h1>
<p>Engage the Override! <code>-qoverride-limts</code></p>
<table>
<thead>
<tr class="header">
<th>Platform</th>
<th>Runtime</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intel</td>
<td>2.35s â 1.60s</td>
<td>1.46x</td>
</tr>
<tr class="even">
<td>AMD</td>
<td>3.13s â 1.66s</td>
<td>1.89x</td>
</tr>
</tbody>
</table>
<p>Sustainable solution is to reduce size of <code>btstep()</code></p>
</section>

<section id="vertical-viscosity" class="title-slide slide level1">
<h1>Vertical Viscosity</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="1248

" data-end-line="1270

" data-line-numbers="1|22

" data-startFrom="1248

">  do K=nz,2,-1 ; do i=is,ie ; if (do_i(i)) then
    !    botfn determines when a point is within the influence of the bottom
    !  boundary layer, going from 1 at the bottom to 0 in the interior.
    z2 = z_i(i,k)
    botfn = 1.0 / (1.0 + 0.09*z2*z2*z2*z2*z2*z2)

    if (CS%bottomdraglaw) then
      Kv_tot(i,K) = Kv_tot(i,K) + (kv_bbl(i) - CS%Kv)*botfn
      r = 0.5*(hvel(i,k) + hvel(i,k-1))
      if (r &gt; bbl_thick(i)) then
        h_shear = ((1.0 - botfn) * r + botfn*bbl_thick(i)) + h_neglect
      else
        h_shear = r + h_neglect
      endif
    else
      Kv_tot(i,K) = Kv_tot(i,K) + (CS%Kvbbl-CS%Kv)*botfn
      h_shear = 0.5*(hvel(i,k) + hvel(i,k-1) + h_neglect)
    endif

    ! Calculate the coupling coefficients from the viscosities.
    a_cpl(i,K) = Kv_tot(i,K) / (h_shear*GV%H_to_Z + I_amax*Kv_tot(i,K))
  endif ; enddo ; enddo ! i &amp; k loops
</code></pre>
<aside class="notes">
<p>The <code>do_I(:)</code> arrays are used to skip over land. These can impede vectorization.</p>
</aside>
</section>

<section id="vertical-viscosity-1" class="title-slide slide level1">
<h1>Vertical Viscosity</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="1462

" data-end-line="1489

" data-line-numbers="1-4|23-27

" data-startFrom="1462

">  do K=nz,2,-1 ; do i=is,ie
    z2 = z_i(i,k)
    botfn_2d(i,k) = 1.0 / (1.0 + 0.09*z2*z2*z2*z2*z2*z2)
  enddo ; enddo ! i &amp; k loops

  if (CS%bottomdraglaw) then
    do K=nz,2,-1 ; do i=is,ie
      Kv_tot(i,K) = Kv_tot(i,K) + (kv_bbl(i) - CS%Kv) * botfn_2d(i,k)
      r = 0.5*(hvel(i,k) + hvel(i,k-1))
      if (r &gt; bbl_thick(i)) then
        h_shear_2d(i,k) = ((1.0 - botfn_2d(i,k)) * r + botfn_2d(i,k) * bbl_thick(i)) + h_neglect
      else
        h_shear_2d(i,k) = r + h_neglect
      endif
    enddo ; enddo ! i &amp; k loops
  else
    do K=nz,2,-1 ; do i=is,ie
      Kv_tot(i,K) = Kv_tot(i,K) + (CS%Kvbbl - CS%Kv) * botfn_2d(i,k)
      h_shear_2d(i,k) = 0.5 * (hvel(i,k) + hvel(i,k-1) + h_neglect)
    enddo ; enddo ! i &amp; k loops
  endif

  do K=nz,2,-1 ; do i=is,ie
    ! Calculate the coupling coefficients from the viscosities.
    a_cpl(i,K) = i_mask(i) * Kv_tot(i,K) &amp;
        / (h_shear_2d(i,k) * GV%H_to_Z + I_amax * Kv_tot(i,K))
  enddo ; enddo ! i &amp; k loops
</code></pre>
<p>Remove <span class="title-ref">do_i(:)</span> to improve pipeline</p>
<aside class="notes">
<p>So we remove them, then multiply by i_mask when writing a_cpl.</p>
<p>This lets us keep moving contiguously though memory, and set up an uninterrupted pipeline.</p>
</aside>
</section>

<section id="maintain-pipelines" class="title-slide slide level1">
<h1>Maintain Pipelines</h1>
<p><img data-src="img/stay_on_target.gif" style="width:45.0%" alt="image" /></p>
<p>Don't stop, keep going, even over land!</p>
<aside class="notes">
<p>We abstract our data as multidimensional, but of course in practice it is still a single 1D array.</p>
<p>To get the best performance, stay "in the trench" and maintain contiguity</p>
</aside>
</section>

<section id="vertical-viscosity-gflops" class="title-slide slide level1">
<h1>Vertical Viscosity GFLOP/s</h1>
<table>
<thead>
<tr class="header">
<th>Subroutine</th>
<th>GFLOP/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vertvisc_coef</td>
<td>1.29 â 2.00</td>
</tr>
<tr class="even">
<td>find_coupling_coef</td>
<td>2.50 â 5.72</td>
</tr>
</tbody>
</table>
<p>FLOPs are up!</p>
</section>

<section id="vertical-viscosity-speedup" class="title-slide slide level1">
<h1>Vertical Viscosity "speedup"</h1>
<table>
<thead>
<tr class="header">
<th>Subroutine</th>
<th>Runtime</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vertvisc_coef</td>
<td>1.54s â 1.50s</td>
<td>ð</td>
</tr>
<tr class="even">
<td>find_coupling_coef</td>
<td>0.82s â 0.68s</td>
<td>1.21x</td>
</tr>
</tbody>
</table>
<p>Runtimes, not so much</p>
<aside class="notes">
<p>Though to tell the truth... the results were less than impressive.</p>
<p>In practice, this could end up being better or worse, depending on the amount of land.</p>
<p>Which is why I haven't submitted this change. But it is interesting an worth revisiting soon.</p>
</aside>
</section>

<section id="what-about-that-eos" class="title-slide slide level1">
<h1>What about that EOS?</h1>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="578

" data-end-line="619

" data-line-numbers="9-14|21-37

" data-startFrom="578

">  if (present(inty_dpa)) then ; do J=Jsq,Jeq ; do i=is,ie
    ! hWght is the distance measure by which the cell is violation of
    ! hydrostatic consistency. For large hWght we bias the interpolation of
    ! T &amp; S along the top and bottom integrals, akin to thickness weighting.
    hWght = 0.0
    if (do_massWeight) &amp;
      hWght = max(0., -bathyT(i,j)-z_t(i,j+1), -bathyT(i,j+1)-z_t(i,j))
    if (hWght &gt; 0.) then
      hL = (z_t(i,j) - z_b(i,j)) + dz_neglect
      hR = (z_t(i,j+1) - z_b(i,j+1)) + dz_neglect
      hWght = hWght * ( (hL-hR)/(hL+hR) )**2
      iDenom = 1.0 / ( hWght*(hR + hL) + hL*hR )
      hWt_LL = (hWght*hL + hR*hL) * iDenom ; hWt_LR = (hWght*hR) * iDenom
      hWt_RR = (hWght*hR + hR*hL) * iDenom ; hWt_RL = (hWght*hL) * iDenom
    else
      hWt_LL = 1.0 ; hWt_LR = 0.0 ; hWt_RR = 1.0 ; hWt_RL = 0.0
    endif

    intz(1) = dpa(i,j) ; intz(5) = dpa(i,j+1)
    do m=2,4
      wt_L = 0.25*real(5-m) ; wt_R = 1.0-wt_L
      wtT_L = wt_L*hWt_LL + wt_R*hWt_RL ; wtT_R = wt_L*hWt_LR + wt_R*hWt_RR

      al0 = wtT_L*al0_2d(i,j) + wtT_R*al0_2d(i,j+1)
      p0 = wtT_L*p0_2d(i,j) + wtT_R*p0_2d(i,j+1)
      lambda = wtT_L*lambda_2d(i,j) + wtT_R*lambda_2d(i,j+1)

      dz = wt_L*(z_t(i,j) - z_b(i,j)) + wt_R*(z_t(i,j+1) - z_b(i,j+1))
      p_ave = -0.5*GxRho*(wt_L*(z_t(i,j)+z_b(i,j)) + &amp;
                          wt_R*(z_t(i,j+1)+z_b(i,j+1)))

      I_al0 = 1.0 / al0
      I_Lzz = 1.0 / (p0 + (lambda * I_al0) + p_ave)
      eps = 0.5*GxRho*dz*I_Lzz ; eps2 = eps*eps

      intz(m) = Pa_to_RL2_T2 * ( g_Earth*dz*((p0 + p_ave)*(I_Lzz*I_al0) - rho_ref_mks) - 2.0*eps * &amp;
                I_Rho * (lambda * I_al0**2) * eps2 * (C1_3 + eps2*(0.2 + eps2*(C1_7 + C1_9*eps2))) )
    enddo
    ! Use Boole&#39;s rule to integrate the values.
    inty_dpa(i,j) = C1_90*(7.0*(intz(1)+intz(5)) + 32.0*(intz(2)+intz(4)) + 12.0*intz(3))
  enddo ; enddo ; endif
</code></pre>
<p>High <em>arithmetic intensity</em>: many ops per memory access</p>
</section>

<section>
<section id="gaea-speedup" class="title-slide slide level1">
<h1>Gaea Speedup</h1>
<p><img data-src="img/subroutine_speedup_gaea.svg" style="width:60.0%" alt="image" /></p>
<p>Overall speedup of ~10%</p>
<aside class="notes">
<p>So several subroutines had significant speedup, maybe about 2x in some cases, a fair bit less in others.</p>
<p>Total improvement was maybe ~10%</p>
<p>But the long tail was always going to make it a long fight to get any speedup. The real goal here was to lay down the rules to maintain efficient code.</p>
<p>In that sense, I think this project has been useful and ought to yield more improvements in the future.</p>
</aside>
</section>
<section id="amd-speedup" class="slide level2">
<h2>AMD Speedup</h2>
<p><img data-src="img/subroutine_speedup.svg" alt="image" /></p>
</section></section>
<section>
<section id="high-resolution-speedup" class="title-slide slide level1">
<h1>High-resolution Speedup</h1>
<p>1440 x 1080 x 75 points, 960 MPI ranks</p>
<table>
<thead>
<tr class="header">
<th>Subroutine</th>
<th>Runtimes</th>
<th>Speedup</th>
<th>(1-core)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>hor_visc</code></td>
<td>1.51s â 0.81s</td>
<td>1.87x</td>
<td>(1.75x)</td>
</tr>
<tr class="even">
<td><code>CorAdCalc</code></td>
<td>0.81s â 0.71s</td>
<td>1.14x</td>
<td>(1.23x)</td>
</tr>
<tr class="odd">
<td><code>btstep</code></td>
<td>4.08s â 3.53s</td>
<td>1.22x</td>
<td>(1.46x)</td>
</tr>
</tbody>
</table>
<p>~7% speedup of dynamic core</p>
</section>
<section id="vertical-viscosity-2" class="slide level2">
<h2>Vertical viscosity</h2>
<table>
<thead>
<tr class="header">
<th>subroutine</th>
<th>Runtimes</th>
<th>Speedup</th>
<th>(1-core)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>vert_visc</code></td>
<td>3.69s â 4.24s</td>
<td>0.87x</td>
<td>(1x)</td>
</tr>
</tbody>
</table>
<p>NOTE: No CPU land masking</p>
</section></section>
<section id="performance-monitoring" class="title-slide slide level1">
<h1>Performance Monitoring</h1>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="odd">
<td><a href="https://github.com/marshallward/MOM6/runs/2640839721?check_suite_focus=true"><img data-src="img/github_fms_clk.png" alt="image" /></a></td>
<td><ul>
<li>Run as regression test</li>
<li>Integrate w/ GitLab CI</li>
<li>Archive resuts to DB</li>
</ul></td>
</tr>
</tbody>
</table>
</section>

<section id="resource-monitoring" class="title-slide slide level1">
<h1>Resource Monitoring</h1>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="odd">
<td><a href="https://github.com/marshallward/MOM6/runs/2640839721?check_suite_focus=true"><img data-src="img/github_perf.png" alt="image" /></a></td>
<td><ul>
<li>Expose perf metrics to CI</li>
<li>Challenges abound...</li>
</ul></td>
</tr>
</tbody>
</table>
</section>

<section>
<section id="summary" class="title-slide slide level1">
<h1>Summary</h1>
<ul>
<li>We are monitoring FLOPs and vectorization</li>
<li>Single-core FLOP rates of ~2 GFLOPs</li>
<li>Achievable speedups of ~1.5x - 2x</li>
<li>Translates to large parallel jobs</li>
<li>Infrastructure to monitor performance underway...</li>
</ul>
</section>
<section id="vectorization-process" class="slide level2">
<h2>Vectorization Process</h2>
<dl>
<dt>Phase 1</dt>
<dd><p>Enable vectorization, confirm optimizations</p>
</dd>
<dt>Phase 2</dt>
<dd><p>Minimize RAM access, collocate arrays</p>
</dd>
<dt>Phase 3</dt>
<dd><p>Memory alignment, more stack usage, peel loops</p>
</dd>
<dt>Phase 4</dt>
<dd><p>Redesign to optimize <em>arithmetic intensity</em></p>
</dd>
</dl>
</section>
<section id="guidelines-for-contributors" class="slide level2">
<h2>Guidelines for Contributors</h2>
<ul>
<li>Avoid if-blocks in loops</li>
<li>Keep references close together</li>
<li>Minimize array reads/writes</li>
<li>Keep functions small (&lt;2000 lines?)</li>
</ul>
</section></section>
<section id="apocrypha" class="title-slide slide level1">
<h1>Apocrypha</h1>

</section>

<section>
<section id="whence-gpus" class="title-slide slide level1">
<h1>Whence GPUs?</h1>
<p>One core vs 1 GPU task</p>
<p><img data-src="img/p1_gpu_flops.svg" style="width:60.0%" alt="image" /></p>
</section>
<section id="whence-gpus-1" class="slide level2">
<h2>Whence GPUs?</h2>
<p>6 cores vs 6 GPU tasks</p>
<p><img data-src="img/p6_gpu_flops.svg" style="width:60.0%" alt="image" /></p>
</section></section>
<section>
<section id="barotropic-index-reorder" class="title-slide slide level1">
<h1>Barotropic index reorder</h1>
<p>Lots of layered sums, e.g.</p>
<pre class="fortran"><code>do j=js,je
   do k=1,nz
      do I=Isq,Ieq
         BT_force_u(I,j) = BT_force_u(I,j) &amp;
               + wt_u(I,j,k) * bc_accel_u(I,j,k)
      enddo
   enddo
enddo</code></pre>
<p>Very non-contiguous! Reorder to (k,i,j)</p>
</section>
<section id="barotropic-kij-speedup" class="slide level2">
<h2>Barotropic kij Speedup</h2>
<p><img data-src="img/subroutine_speedup_bt.svg" style="width:70.0%" alt="image" /></p>
</section>
<section id="reordering-problems" class="slide level2">
<h2>Reordering problems</h2>
<ol type="1">
<li><p>Answer changes:</p>
<p><span class="math display">\[(u_1 + u_2 + u_3 + u_4) + (u_5 + u_6 + u_7 + u_8) + ...\]</span></p></li>
<li><p>Transpose needed outside of loops! Very slow...</p>
<pre class="fortran"><code>do k=1,nz
  u_tr(k,:,:) = u(:,:,k)
  v_tr(k,:,:) = v(:,:,k)
  ! ...
enddo</code></pre></li>
</ol>
<p>Work in progress...</p>
</section></section>
<section id="answer-changing-speedups" class="title-slide slide level1">
<h1>Answer-changing Speedups</h1>
<p>This is slow:</p>
<pre><code>botfn_2d(i,k) = 1.0 / (1.0 + 0.09*z2*z2*z2*z2*z2*z2)</code></pre>
<p>This is faster:</p>
<pre><code>z2_sq = z2 * z2
botfn_2d(i,k) = 1.0 / (1.0 + 0.09 * (z2_sq * z2_sq * z2_sq))</code></pre>
<ol type="1">
<li>Fewer operations (3 vs 5 mults)</li>
<li>Fewer instructions: Two can be pipelined</li>
</ol>
<p>... but changes answers</p>
</section>

<section id="excessive-stack-1" class="title-slide slide level1">
<h1>Excessive Stack?</h1>
<p><code>horizontal_viscosity</code> has 18 3D arrays on stack! And over 2x as many 2D arrays...</p>
<pre class="fortran numberLines" data-code="fortran

"><code data-start-line="1

" data-startFrom="578

">subroutine horizontal_viscosity(u, v, h, diffu, diffv, MEKE, VarMix, G, GV, US, &amp;
                                CS, OBC, BT, TD, ADp)
  type(ocean_grid_type),         intent(in)  :: G      !&lt; The ocean&#39;s grid structure.
  type(verticalGrid_type),       intent(in)  :: GV     !&lt; The ocean&#39;s vertical grid structure.
  real, dimension(SZIB_(G),SZJ_(G),SZK_(GV)), &amp;
                                 intent(in)  :: u      !&lt; The zonal velocity [L T-1 ~&gt; m s-1].
  real, dimension(SZI_(G),SZJB_(G),SZK_(GV)), &amp;
                                 intent(in)  :: v      !&lt; The meridional velocity [L T-1 ~&gt; m s-1].
  real, dimension(SZI_(G),SZJ_(G),SZK_(GV)), &amp;
                                 intent(inout) :: h    !&lt; Layer thicknesses [H ~&gt; m or kg m-2].
  real, dimension(SZIB_(G),SZJ_(G),SZK_(GV)), &amp;
                                 intent(out) :: diffu  !&lt; Zonal acceleration due to convergence of
                                                       !! along-coordinate stress tensor [L T-2 ~&gt; m s-2]
  real, dimension(SZI_(G),SZJB_(G),SZK_(GV)), &amp;
                                 intent(out) :: diffv  !&lt; Meridional acceleration due to convergence
                                                       !! of along-coordinate stress tensor [L T-2 ~&gt; m s-2].
  type(MEKE_type),               pointer     :: MEKE   !&lt; Pointer to a structure containing fields
                                                       !! related to Mesoscale Eddy Kinetic Energy.
  type(VarMix_CS),               pointer     :: VarMix !&lt; Pointer to a structure with fields that
                                                       !! specify the spatially variable viscosities
  type(unit_scale_type),         intent(in)  :: US     !&lt; A dimensional unit scaling type
  type(hor_visc_CS),             pointer     :: CS     !&lt; Control structure returned by a previous
                                                       !! call to hor_visc_init.
  type(ocean_OBC_type), optional, pointer    :: OBC    !&lt; Pointer to an open boundary condition type
  type(barotropic_CS),  optional, pointer    :: BT     !&lt; Pointer to a structure containing
                                                       !! barotropic velocities.
  type(thickness_diffuse_CS), optional, pointer :: TD  !&lt; Pointer to a structure containing
                                                       !! thickness diffusivities.
  type(accel_diag_ptrs), optional, pointer :: ADp      !&lt; Acceleration diagnostic pointers

  ! Local variables
  real, dimension(SZIB_(G),SZJ_(G)) :: &amp;
    Del2u, &amp;      ! The u-compontent of the Laplacian of velocity [L-1 T-1 ~&gt; m-1 s-1]
    h_u, &amp;        ! Thickness interpolated to u points [H ~&gt; m or kg m-2].
    vort_xy_dy, &amp; ! y-derivative of vertical vorticity (d/dy(dv/dx - du/dy)) [L-1 T-1 ~&gt; m-1 s-1]
    div_xx_dx, &amp;  ! x-derivative of horizontal divergence (d/dx(du/dx + dv/dy)) [L-1 T-1 ~&gt; m-1 s-1]
    ubtav         ! zonal barotropic vel. ave. over baroclinic time-step [L T-1 ~&gt; m s-1]
  real, dimension(SZI_(G),SZJB_(G)) :: &amp;
    Del2v, &amp;      ! The v-compontent of the Laplacian of velocity [L-1 T-1 ~&gt; m-1 s-1]
    h_v, &amp;        ! Thickness interpolated to v points [H ~&gt; m or kg m-2].
    vort_xy_dx, &amp; ! x-derivative of vertical vorticity (d/dx(dv/dx - du/dy)) [L-1 T-1 ~&gt; m-1 s-1]
    div_xx_dy, &amp;  ! y-derivative of horizontal divergence (d/dy(du/dx + dv/dy)) [L-1 T-1 ~&gt; m-1 s-1]
    vbtav         ! meridional barotropic vel. ave. over baroclinic time-step [L T-1 ~&gt; m s-1]
  real, dimension(SZI_(G),SZJ_(G)) :: &amp;
    dudx_bt, dvdy_bt, &amp; ! components in the barotropic horizontal tension [T-1 ~&gt; s-1]
    div_xx, &amp;     ! Estimate of horizontal divergence at h-points [T-1 ~&gt; s-1]
    sh_xx, &amp;      ! horizontal tension (du/dx - dv/dy) including metric terms [T-1 ~&gt; s-1]
    sh_xx_bt, &amp;   ! barotropic horizontal tension (du/dx - dv/dy) including metric terms [T-1 ~&gt; s-1]
    str_xx,&amp;      ! str_xx is the diagonal term in the stress tensor [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    str_xx_GME,&amp;  ! smoothed diagonal term in the stress tensor from GME [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    bhstr_xx, &amp;   ! A copy of str_xx that only contains the biharmonic contribution [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    FrictWorkIntz, &amp; ! depth integrated energy dissipated by lateral friction [R L2 T-3 ~&gt; W m-2]
    grad_vort_mag_h, &amp; ! Magnitude of vorticity gradient at h-points [L-1 T-1 ~&gt; m-1 s-1]
    grad_vort_mag_h_2d, &amp; ! Magnitude of 2d vorticity gradient at h-points [L-1 T-1 ~&gt; m-1 s-1]
    Del2vort_h, &amp; ! Laplacian of vorticity at h-points [L-2 T-1 ~&gt; m-2 s-1]
    grad_div_mag_h, &amp;     ! Magnitude of divergence gradient at h-points [L-1 T-1 ~&gt; m-1 s-1]
    dudx, dvdy, &amp;    ! components in the horizontal tension [T-1 ~&gt; s-1]
    grad_vel_mag_h, &amp; ! Magnitude of the velocity gradient tensor squared at h-points [T-2 ~&gt; s-2]
    grad_vel_mag_bt_h, &amp; ! Magnitude of the barotropic velocity gradient tensor squared at h-points [T-2 ~&gt; s-2]
    grad_d2vel_mag_h, &amp; ! Magnitude of the Laplacian of the velocity vector, squared [L-2 T-2 ~&gt; m-2 s-2]
    boundary_mask_h ! A mask that zeroes out cells with at least one land edge [nondim]

  real, dimension(SZIB_(G),SZJB_(G)) :: &amp;
    dvdx, dudy, &amp; ! components in the shearing strain [T-1 ~&gt; s-1]
    dDel2vdx, dDel2udy, &amp; ! Components in the biharmonic equivalent of the shearing strain [L-2 T-1 ~&gt; m-2 s-1]
    dvdx_bt, dudy_bt,   &amp; ! components in the barotropic shearing strain [T-1 ~&gt; s-1]
    sh_xy,  &amp;     ! horizontal shearing strain (du/dy + dv/dx) including metric terms [T-1 ~&gt; s-1]
    sh_xy_bt, &amp;   ! barotropic horizontal shearing strain (du/dy + dv/dx) inc. metric terms [T-1 ~&gt; s-1]
    str_xy, &amp;     ! str_xy is the cross term in the stress tensor [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    str_xy_GME, &amp; ! smoothed cross term in the stress tensor from GME [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    bhstr_xy, &amp;   ! A copy of str_xy that only contains the biharmonic contribution [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
    vort_xy, &amp;    ! Vertical vorticity (dv/dx - du/dy) including metric terms [T-1 ~&gt; s-1]
    Leith_Kh_q, &amp; ! Leith Laplacian viscosity at q-points [L2 T-1 ~&gt; m2 s-1]
    Leith_Ah_q, &amp; ! Leith bi-harmonic viscosity at q-points [L4 T-1 ~&gt; m4 s-1]
    grad_vort_mag_q, &amp; ! Magnitude of vorticity gradient at q-points [L-1 T-1 ~&gt; m-1 s-1]
    grad_vort_mag_q_2d, &amp; ! Magnitude of 2d vorticity gradient at q-points [L-1 T-1 ~&gt; m-1 s-1]
    Del2vort_q, &amp; ! Laplacian of vorticity at q-points [L-2 T-1 ~&gt; m-2 s-1]
    grad_div_mag_q, &amp;  ! Magnitude of divergence gradient at q-points [L-1 T-1 ~&gt; m-1 s-1]
    grad_vel_mag_q, &amp;  ! Magnitude of the velocity gradient tensor squared at q-points [T-2 ~&gt; s-2]
    hq, &amp;         ! harmonic mean of the harmonic means of the u- &amp; v point thicknesses [H ~&gt; m or kg m-2]
                  ! This form guarantees that hq/hu &lt; 4.
    grad_vel_mag_bt_q, &amp;  ! Magnitude of the barotropic velocity gradient tensor squared at q-points [T-2 ~&gt; s-2]
    boundary_mask_q ! A mask that zeroes out cells with at least one land edge [nondim]

  real, dimension(SZIB_(G),SZJB_(G),SZK_(GV)) :: &amp;
    Ah_q, &amp;      ! biharmonic viscosity at corner points [L4 T-1 ~&gt; m4 s-1]
    Kh_q, &amp;      ! Laplacian viscosity at corner points [L2 T-1 ~&gt; m2 s-1]
    vort_xy_q, &amp; ! vertical vorticity at corner points [T-1 ~&gt; s-1]
    sh_xy_q,   &amp; ! horizontal shearing strain at corner points [T-1 ~&gt; s-1]
    GME_coeff_q, &amp;  !&lt; GME coeff. at q-points [L2 T-1 ~&gt; m2 s-1]
    ShSt         ! A diagnostic array of shear stress [T-1 ~&gt; s-1].
  real, dimension(SZIB_(G),SZJ_(G),SZK_(GV)+1) :: &amp;
    KH_u_GME  !&lt; interface height diffusivities in u-columns [L2 T-1 ~&gt; m2 s-1]
  real, dimension(SZI_(G),SZJB_(G),SZK_(GV)+1) :: &amp;
    KH_v_GME  !&lt; interface height diffusivities in v-columns [L2 T-1 ~&gt; m2 s-1]
  real, dimension(SZI_(G),SZJ_(G),SZK_(GV)) :: &amp;
    Ah_h, &amp;          ! biharmonic viscosity at thickness points [L4 T-1 ~&gt; m4 s-1]
    Kh_h, &amp;          ! Laplacian viscosity at thickness points [L2 T-1 ~&gt; m2 s-1]
    FrictWork, &amp;     ! work done by MKE dissipation mechanisms [R L2 T-3 ~&gt; W m-2]
    FrictWork_GME, &amp; ! work done by GME [R L2 T-3 ~&gt; W m-2]
    div_xx_h,      &amp; ! horizontal divergence [T-1 ~&gt; s-1]
    sh_xx_h,       &amp; ! horizontal tension (du/dx - dv/dy) including metric terms [T-1 ~&gt; s-1]
    NoSt             ! A diagnostic array of normal stress [T-1 ~&gt; s-1].
  real, dimension(SZI_(G),SZJ_(G),SZK_(G)) :: &amp;
    grid_Re_Kh, &amp;    ! Grid Reynolds number for Laplacian horizontal viscosity at h points [nondim]
    grid_Re_Ah, &amp;    ! Grid Reynolds number for Biharmonic horizontal viscosity at h points [nondim]
    GME_coeff_h      ! GME coeff. at h-points [L2 T-1 ~&gt; m2 s-1]
  real :: AhSm       ! Smagorinsky biharmonic viscosity [L4 T-1 ~&gt; m4 s-1]
  real :: AhLth      ! 2D Leith biharmonic viscosity [L4 T-1 ~&gt; m4 s-1]
  real :: mod_Leith  ! nondimensional coefficient for divergence part of modified Leith
                     ! viscosity. Here set equal to nondimensional Laplacian Leith constant.
                     ! This is set equal to zero if modified Leith is not used.
  real :: Shear_mag_bc  ! Shear_mag value in backscatter [T-1 ~&gt; s-1]
  real :: sh_xx_sq   ! Square of tension (sh_xx) [T-2 ~&gt; s-2]
  real :: sh_xy_sq   ! Square of shearing strain (sh_xy) [T-2 ~&gt; s-2]
  real :: h2uq, h2vq ! temporary variables [H2 ~&gt; m2 or kg2 m-4].
  real :: hu, hv     ! Thicknesses interpolated by arithmetic means to corner
                     ! points; these are first interpolated to u or v velocity
                     ! points where masks are applied [H ~&gt; m or kg m-2].
  real :: h_neglect  ! thickness so small it can be lost in roundoff and so neglected [H ~&gt; m or kg m-2]
  real :: h_neglect3 ! h_neglect^3 [H3 ~&gt; m3 or kg3 m-6]
  real :: h_min      ! Minimum h at the 4 neighboring velocity points [H ~&gt; m]
  real :: Kh_scale  ! A factor between 0 and 1 by which the horizontal
                    ! Laplacian viscosity is rescaled [nondim]
  real :: RoScl     ! The scaling function for MEKE source term [nondim]
  real :: FatH      ! abs(f) at h-point for MEKE source term [T-1 ~&gt; s-1]
  real :: local_strain ! Local variable for interpolating computed strain rates [T-1 ~&gt; s-1].
  real :: meke_res_fn ! A copy of the resolution scaling factor if being applied to MEKE. Otherwise =1.
  real :: GME_coeff ! The GME (negative) viscosity coefficient [L2 T-1 ~&gt; m2 s-1]
  real :: GME_coeff_limiter ! Maximum permitted value of the GME coefficient [L2 T-1 ~&gt; m2 s-1]
  real :: FWfrac    ! Fraction of maximum theoretical energy transfer to use when scaling GME coefficient [nondim]
  real :: DY_dxBu   ! Ratio of meridional over zonal grid spacing at vertices [nondim]
  real :: DX_dyBu   ! Ratio of zonal over meridiononal grid spacing at vertices [nondim]
  real :: DY_dxCv   ! Ratio of meridional over zonal grid spacing at faces [nondim]
  real :: DX_dyCu   ! Ratio of zonal over meridional grid spacing at faces [nondim]
  real :: Sh_F_pow  ! The ratio of shear over the absolute value of f raised to some power and rescaled [nondim]
  real :: backscat_subround ! The ratio of f over Shear_mag that is so small that the backscatter
                    ! calculation gives the same value as if f were 0 [nondim].
  real :: H0_GME    ! Depth used to scale down GME coefficient in shallow areas [Z ~&gt; m]
  real :: KE        ! Local kinetic energy [L2 T-2 ~&gt; m2 s-2]
  real :: d_del2u   ! dy-weighted Laplacian(u) diff in x [L-2 T-1 ~&gt; m-2 s-1]
  real :: d_del2v   ! dx-weighted Laplacian(v) diff in y [L-2 T-1 ~&gt; m-2 s-1]
  real :: d_str     ! Stress tensor update [H L2 T-2 ~&gt; m3 s-2 or kg s-2]
  real :: grad_vort ! Vorticity gradient magnitude [L-1 T-1 ~&gt; m-1 s-1]
  real :: grad_vort_qg ! QG-based vorticity gradient magnitude [L-1 T-1 ~&gt; m-1 s-1]
  real :: grid_Kh   ! Laplacian viscosity bound by grid [L2 T-1 ~&gt; m2 s-1]
  real :: grid_Ah   ! Biharmonic viscosity bound by grid [L4 T-1 ~&gt; m4 s-1]

  logical :: rescale_Kh, legacy_bound
  logical :: find_FrictWork
  logical :: apply_OBC = .false.
  logical :: use_MEKE_Ku
  logical :: use_MEKE_Au
  integer :: is, ie, js, je, Isq, Ieq, Jsq, Jeq, nz
  integer :: i, j, k, n
  real :: inv_PI3, inv_PI2, inv_PI6

  ! Fields evaluated on active layers, used for constructing 3D stress fields
  ! NOTE: The position of these declarations can impact performance, due to the
  !   very large number of stack arrays in this function.  Move with caution!
  real, dimension(SZIB_(G),SZJB_(G)) :: &amp;
    Ah, &amp;           ! biharmonic viscosity (h or q) [L4 T-1 ~&gt; m4 s-1]
    Kh, &amp;           ! Laplacian  viscosity [L2 T-1 ~&gt; m2 s-1]
    Shear_mag, &amp;    ! magnitude of the shear [T-1 ~&gt; s-1]
    vert_vort_mag, &amp;  ! magnitude of the vertical vorticity gradient [L-1 T-1 ~&gt; m-1 s-1]
    hrat_min, &amp;     ! h_min divided by the thickness at the stress point (h or q) [nondim]
    visc_bound_rem  ! fraction of overall viscous bounds that remain to be applied [nondim]

  real, dimension(SZIB_(G),SZJ_(G)) :: &amp;
    hf_diffu_2d, &amp;    ! Depth sum of hf_diffu, hf_diffv [L T-2 ~&gt; m s-2]
    intz_diffu_2d     ! Depth-integral of diffu [L2 T-2 ~&gt; m2 s-2]

  real, dimension(SZI_(G),SZJB_(G)) :: &amp;
    hf_diffv_2d, &amp;    ! Depth sum of hf_diffu, hf_diffv [L T-2 ~&gt; m s-2]
    intz_diffv_2d     ! Depth-integral of diffv [L2 T-2 ~&gt; m2 s-2]

</code></pre>
<p>Moving declarations will slow model! This is worrying...</p>
</section>
    </div>
  </div>

  <script src="./reveal.js/dist/reveal.js"></script>
  <script src="./reveal.js/plugin/math/math.js"></script>
  <script src="./reveal.js/plugin/notes/notes.js"></script>
  <script src="./reveal.js/plugin/highlight/highlight.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          //TeX: {
          //  inlineMath: [['\\(','\\)']],
          //  displayMath: [['\\[','\\]']],
          //  balanceBraces: true,
          //  processEscapes: false,
          //  processRefs: true,
          //  processEnvironments: true,
          //  preview: 'TeX',
          //  skipTags: ['script','noscript','style','textarea','pre','code'],
          //  ignoreClass: 'tex2jax_ignore',
          //  processClass: 'tex2jax_process'
          //},
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: './reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: './reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: './reveal.js/plugin/notes/notes.js', async: true }
        ],
        plugins : [ RevealMath, RevealNotes, RevealHighlight],
      });
    </script>
    </body>
</html>
